{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f29adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "env = gym.make(\"Humanoid-v5\")   \n",
    "obs, info = env.reset(seed=64)\n",
    "obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "input_dim = obs.shape[0]\n",
    "output_dim = env.action_space.shape[0]\n",
    "discount_factor = 0.99\n",
    "learning_rate = 3e-4\n",
    "beta = 0.01\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(512, output_dim)\n",
    "        self.log_std = nn.Parameter(torch.ones(output_dim) * 0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.core(x)\n",
    "        mean = self.mean_head(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        return mean, std\n",
    "\n",
    "policy = PolicyNetwork(input_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6439355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Loss: -0.27843886613845825, Return: 201.49035314197278\n",
      "Episode 200, Loss: -0.2373594343662262, Return: 167.46315598553522\n",
      "Episode 300, Loss: -0.35641148686408997, Return: 201.01219409110644\n",
      "Episode 400, Loss: -0.3101004660129547, Return: 178.82881235460925\n",
      "Episode 500, Loss: -0.282781720161438, Return: 193.19660301935698\n",
      "Episode 600, Loss: -0.3129720091819763, Return: 199.48630163553608\n",
      "Episode 700, Loss: -0.2479310929775238, Return: 181.89244715445545\n",
      "Episode 800, Loss: -0.31906720995903015, Return: 236.2175380359367\n",
      "Episode 900, Loss: -0.3369368612766266, Return: 191.60266173217747\n",
      "Episode 1000, Loss: -0.2707272171974182, Return: 107.36358023967387\n",
      "Episode 1100, Loss: -0.2768295109272003, Return: 85.21585917127881\n",
      "Episode 1200, Loss: -0.24571041762828827, Return: 175.84734751773934\n",
      "Episode 1300, Loss: -0.24822227656841278, Return: 102.28215925647363\n",
      "Episode 1400, Loss: -0.17905400693416595, Return: 105.40008925213691\n",
      "Episode 1500, Loss: -0.23660974204540253, Return: 145.49644137371365\n",
      "Episode 1600, Loss: -0.21281355619430542, Return: 125.4306679699264\n",
      "Episode 1700, Loss: -0.18240678310394287, Return: 100.68934043655095\n",
      "Episode 1800, Loss: -0.2816214859485626, Return: 145.90557113944652\n",
      "Episode 1900, Loss: -0.26595064997673035, Return: 163.79071929803246\n",
      "Episode 2000, Loss: -0.23020721971988678, Return: 126.54102186909934\n",
      "Episode 2100, Loss: -0.23398691415786743, Return: 131.92073507944386\n",
      "Episode 2200, Loss: -0.31570231914520264, Return: 189.62352926280215\n",
      "Episode 2300, Loss: -0.3251968026161194, Return: 134.71925261618156\n",
      "Episode 2400, Loss: -0.2856893837451935, Return: 191.01635509644845\n",
      "Episode 2500, Loss: -0.2553468942642212, Return: 131.29251457854795\n",
      "Episode 2600, Loss: -0.27590787410736084, Return: 137.05557515956505\n",
      "Episode 2700, Loss: -0.2521135210990906, Return: 152.58573045809044\n",
      "Episode 2800, Loss: -0.30751901865005493, Return: 137.42788011675296\n",
      "Episode 2900, Loss: -0.3021475374698639, Return: 164.15593346761906\n",
      "Episode 3000, Loss: -0.2031562775373459, Return: 193.76148548362465\n",
      "Episode 3100, Loss: -0.2689942717552185, Return: 142.7810165879854\n",
      "Episode 3200, Loss: -0.3104833960533142, Return: 150.63841738315148\n",
      "Episode 3300, Loss: -0.2674461901187897, Return: 240.66318289711822\n",
      "Episode 3400, Loss: -0.27147507667541504, Return: 157.7978117991387\n",
      "Episode 3500, Loss: -0.2541908323764801, Return: 123.29208364919593\n",
      "Episode 3600, Loss: -0.32395321130752563, Return: 162.2484833299561\n",
      "Episode 3700, Loss: -0.35224536061286926, Return: 141.8824130117245\n",
      "Episode 3800, Loss: -0.29288825392723083, Return: 134.53860609664497\n",
      "Episode 3900, Loss: -0.2862202525138855, Return: 148.47527244402275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m observations.append(obs)\n\u001b[32m     18\u001b[39m actions.append(raw_action)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m obs, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclamped_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(terminated, truncated)\u001b[39;00m\n\u001b[32m     21\u001b[39m obs = torch.tensor(obs, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\humanoid_v5.py:497\u001b[39m, in \u001b[36mHumanoidEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    485\u001b[39m info = {\n\u001b[32m    486\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mx_position\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.data.qpos[\u001b[32m0\u001b[39m],\n\u001b[32m    487\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my_position\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.data.qpos[\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m     **reward_info,\n\u001b[32m    494\u001b[39m }\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_env.py:159\u001b[39m, in \u001b[36mMujocoEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    156\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m    Render a frame from the MuJoCo simulation as specified by the render_mode.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmujoco_renderer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:736\u001b[39m, in \u001b[36mMujocoRenderer.render\u001b[39m\u001b[34m(self, render_mode)\u001b[39m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m viewer.render(render_mode=render_mode, camera_id=\u001b[38;5;28mself\u001b[39m.camera_id)\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:458\u001b[39m, in \u001b[36mWindowViewer.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    456\u001b[39m         \u001b[38;5;28mself\u001b[39m._loop_count = \u001b[32m1\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop_count > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m         \u001b[38;5;28mself\u001b[39m._loop_count -= \u001b[32m1\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;66;03m# clear overlay\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\mujoco\\mujoco_rendering.py:430\u001b[39m, in \u001b[36mWindowViewer.render.<locals>.update\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._hide_menu:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gridpos, [t1, t2] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._overlays.items():\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         \u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjr_overlay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmujoco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjtFontScale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmjFONTSCALE_150\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgridpos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m glfw.swap_buffers(\u001b[38;5;28mself\u001b[39m.window)\n\u001b[32m    440\u001b[39m glfw.poll_events()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "episodes = 10000\n",
    "total_loss = 0\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    obs = torch.tensor(obs, dtype=torch.float32)\n",
    "    done = False\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            mean, std = policy(obs)\n",
    "            dist = Normal(mean, std)\n",
    "            raw_action = dist.sample()\n",
    "            clamped_action = torch.clamp(raw_action, -1, 1)\n",
    "            observations.append(obs)\n",
    "            actions.append(raw_action)\n",
    "            obs, reward, terminated, truncated, info = env.step(clamped_action.detach().numpy())\n",
    "            # print(terminated, truncated)\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "        \n",
    "    \n",
    "    discounted_rewards = []\n",
    "    total_reward = 0\n",
    "    for r in reversed(rewards):\n",
    "        total_reward = r + discount_factor * total_reward\n",
    "        discounted_rewards.append(total_reward)\n",
    "    discounted_rewards.reverse()\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "    Gs = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n",
    "    # env.render()\n",
    "    loss = 0\n",
    "    \n",
    "    for obs, reward, action in zip(observations, Gs, actions):\n",
    "        mean, std = policy(obs)\n",
    "        dist = Normal(mean, std)\n",
    "        log_probs = dist.log_prob(action).sum()\n",
    "        entropy = dist.entropy().sum()\n",
    "        loss += -log_probs * reward - beta * entropy\n",
    "    loss = loss / len(observations)\n",
    "\n",
    "    total_loss += loss\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        batch_loss = total_loss / 100\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Episode {episode + 1}, Loss: {batch_loss.item()}, Return: {sum(rewards)}\")\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e9b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Humanoid-v5\", render_mode=\"human\")\n",
    "with torch.no_grad():\n",
    "    obs, info = env.reset(seed =64)\n",
    "    obs = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "    episodes = 10000\n",
    "    total_loss = 0\n",
    "    for episode in range(episodes ):\n",
    "        \n",
    "        obs, info = env.reset(seed=episode)\n",
    "        obs = torch.tensor(obs , dtype=torch.float32)\n",
    "\n",
    "        done = False \n",
    "        observations = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        \n",
    "        while not done:\n",
    "            mean , std = policy(obs)\n",
    "            dist = Normal(mean, std) \n",
    "            action = dist.sample()\n",
    "            action = torch.clamp(action , -0.4 , 0.4)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, reward, terminated , truncated , info = env.step(action.numpy()) \n",
    "            obs = torch.tensor(obs , dtype=torch.float32)\n",
    "            done = terminated or truncated  \n",
    "            rewards.append(reward)\n",
    "            env.render()\n",
    "        discounted_rewards = []\n",
    "        total_reward = 0 \n",
    "\n",
    "        # for i in range(len(rewards)-1, -1 , -1):\n",
    "        #     total_reward = rewards[i] + discount_factor * total_reward \n",
    "        #     discounted_rewards.append(total_reward)\n",
    "        # discounted_rewards.reverse()\n",
    "\n",
    "        # discounted_rewards = torch.tensor(discounted_rewards , dtype=torch.float32)\n",
    "        # Gs = (discounted_rewards - discounted_rewards.mean())/(discounted_rewards.std() + 1e-7)\n",
    "        # loss = 0 \n",
    "\n",
    "        # for obs, reward, action in zip(observations , Gs,actions):\n",
    "        #     mean , std  = policy(obs)\n",
    "        #     dist = Normal(mean , std)\n",
    "        #     log_probs = dist.log_prob(action).sum()\n",
    "        #     entropy = dist.entropy().sum()\n",
    "        #     loss += -log_probs * reward  - beta * entropy \n",
    "            \n",
    "            \n",
    "        # total_loss +=loss \n",
    "        # if (episode+1) % 100 == 0:\n",
    "            \n",
    "        #     total_loss /= 100\n",
    "        #     # optimizer.zero_grad()\n",
    "        #     # total_loss.backward()\n",
    "        #     # optimizer.step()\n",
    "        #     print(f\"Episode {episode + 1}, Loss: {total_loss.item()}\")\n",
    "        #     total_loss = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda9485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
