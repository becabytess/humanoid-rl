{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3facf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m \n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:274\u001b[39m\n\u001b[32m    270\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    272\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:250\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    248\u001b[39m is_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     res = \u001b[43mkernel32\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     last_error = ctypes.get_last_error()\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error != \u001b[32m126\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from copy import deepcopy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Linear(4,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.core(x)\n",
    "\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Linear(4,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.core(x)\n",
    "\n",
    "\n",
    "policyModel = Policy()\n",
    "\n",
    "oldPolicyModel = deepcopy(policyModel)\n",
    "def update_old_policy():\n",
    "    global oldPolicyModel\n",
    "    oldPolicyModel = deepcopy(policyModel)\n",
    "    for param in oldPolicyModel.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "valueModel = Value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711bde48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6464bdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:214: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([34, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([43])) that is different to the input size (torch.Size([43, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1  loss:  tensor([-6.4864e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2  loss:  tensor([-0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  3  loss:  tensor([0.0160], grad_fn=<DivBackward0>)\n",
      "episode:  4  loss:  tensor([-0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  5  loss:  tensor([0.0114], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([70])) that is different to the input size (torch.Size([70, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([45])) that is different to the input size (torch.Size([45, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  6  loss:  tensor([0.0135], grad_fn=<DivBackward0>)\n",
      "episode:  7  loss:  tensor([0.0155], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  8  loss:  tensor([0.0258], grad_fn=<DivBackward0>)\n",
      "episode:  9  loss:  tensor([0.0135], grad_fn=<DivBackward0>)\n",
      "episode:  10  loss:  tensor([-0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  11  loss:  tensor([-6.1467e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([74])) that is different to the input size (torch.Size([74, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  12  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  13  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([44])) that is different to the input size (torch.Size([44, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([54])) that is different to the input size (torch.Size([54, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  14  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  15  loss:  tensor([0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  16  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([59])) that is different to the input size (torch.Size([59, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  17  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  18  loss:  tensor([-0.0027], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([55, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  19  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  20  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  21  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  22  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([35])) that is different to the input size (torch.Size([35, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  23  loss:  tensor([-0.0116], grad_fn=<DivBackward0>)\n",
      "episode:  24  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([33])) that is different to the input size (torch.Size([33, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  25  loss:  tensor([0.0054], grad_fn=<DivBackward0>)\n",
      "episode:  26  loss:  tensor([-0.0087], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([48])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  27  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  28  loss:  tensor([0.0527], grad_fn=<DivBackward0>)\n",
      "episode:  29  loss:  tensor([0.0281], grad_fn=<DivBackward0>)\n",
      "episode:  30  loss:  tensor([0.0196], grad_fn=<DivBackward0>)\n",
      "episode:  31  loss:  tensor([2.4246e-08], grad_fn=<DivBackward0>)\n",
      "episode:  32  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([58])) that is different to the input size (torch.Size([58, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  33  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  34  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([82])) that is different to the input size (torch.Size([82, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  35  loss:  tensor([-0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  36  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  37  loss:  tensor([0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  38  loss:  tensor([-0.0099], grad_fn=<DivBackward0>)\n",
      "episode:  39  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([88])) that is different to the input size (torch.Size([88, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  40  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  41  loss:  tensor([-4.0531e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([69])) that is different to the input size (torch.Size([69, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  42  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  43  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([73, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  44  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  45  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([78])) that is different to the input size (torch.Size([78, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  46  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  47  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  48  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  49  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  50  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  51  loss:  tensor([4.1181e-08], grad_fn=<DivBackward0>)\n",
      "episode:  52  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([51])) that is different to the input size (torch.Size([51, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  53  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  54  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  55  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  56  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  57  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  58  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  59  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  60  loss:  tensor([0.0124], grad_fn=<DivBackward0>)\n",
      "episode:  61  loss:  tensor([4.1723e-08], grad_fn=<DivBackward0>)\n",
      "episode:  62  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  63  loss:  tensor([0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  64  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  65  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([46])) that is different to the input size (torch.Size([46, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  66  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  67  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  68  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([42])) that is different to the input size (torch.Size([42, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([49])) that is different to the input size (torch.Size([49, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n",
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  69  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  70  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  71  loss:  tensor([6.3409e-08], grad_fn=<DivBackward0>)\n",
      "episode:  72  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([76])) that is different to the input size (torch.Size([76, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  73  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  74  loss:  tensor([7.2248e-05], grad_fn=<DivBackward0>)\n",
      "episode:  75  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  76  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  77  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  78  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  79  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  80  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([66])) that is different to the input size (torch.Size([66, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  81  loss:  tensor([-6.0508e-08], grad_fn=<DivBackward0>)\n",
      "episode:  82  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  83  loss:  tensor([1.0273e-06], grad_fn=<DivBackward0>)\n",
      "episode:  84  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  85  loss:  tensor([-5.8984e-06], grad_fn=<DivBackward0>)\n",
      "episode:  86  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  87  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  88  loss:  tensor([-1.3420e-05], grad_fn=<DivBackward0>)\n",
      "episode:  89  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  90  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([39])) that is different to the input size (torch.Size([39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  91  loss:  tensor([-6.1308e-08], grad_fn=<DivBackward0>)\n",
      "episode:  92  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([84])) that is different to the input size (torch.Size([84, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  93  loss:  tensor([-6.4568e-06], grad_fn=<DivBackward0>)\n",
      "episode:  94  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([57])) that is different to the input size (torch.Size([57, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  95  loss:  tensor([6.8483e-05], grad_fn=<DivBackward0>)\n",
      "episode:  96  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  97  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  98  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  99  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  100  loss:  tensor([-5.9978e-05], grad_fn=<DivBackward0>)\n",
      "episode:  101  loss:  tensor([-8.6384e-09], grad_fn=<DivBackward0>)\n",
      "episode:  102  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  103  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  104  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  105  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([86])) that is different to the input size (torch.Size([86, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  106  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  107  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  108  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  109  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  110  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  111  loss:  tensor([-2.0862e-08], grad_fn=<DivBackward0>)\n",
      "episode:  112  loss:  tensor([-5.8260e-06], grad_fn=<DivBackward0>)\n",
      "episode:  113  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  114  loss:  tensor([5.8998e-05], grad_fn=<DivBackward0>)\n",
      "episode:  115  loss:  tensor([8.2177e-05], grad_fn=<DivBackward0>)\n",
      "episode:  116  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  117  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  118  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  119  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  120  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([53])) that is different to the input size (torch.Size([53, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  121  loss:  tensor([-4.1611e-08], grad_fn=<DivBackward0>)\n",
      "episode:  122  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  123  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  124  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  125  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  126  loss:  tensor([-0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  127  loss:  tensor([-0.0129], grad_fn=<DivBackward0>)\n",
      "episode:  128  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  129  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  130  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  131  loss:  tensor([1.7030e-09], grad_fn=<DivBackward0>)\n",
      "episode:  132  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  133  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  134  loss:  tensor([0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  135  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  136  loss:  tensor([-0.0073], grad_fn=<DivBackward0>)\n",
      "episode:  137  loss:  tensor([0.0084], grad_fn=<DivBackward0>)\n",
      "episode:  138  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  139  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  140  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  141  loss:  tensor([-6.6378e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([63])) that is different to the input size (torch.Size([63, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  142  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  143  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  144  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  145  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  146  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  147  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  148  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([67])) that is different to the input size (torch.Size([67, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  149  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  150  loss:  tensor([0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  151  loss:  tensor([-4.6832e-08], grad_fn=<DivBackward0>)\n",
      "episode:  152  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  153  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  154  loss:  tensor([0.0153], grad_fn=<DivBackward0>)\n",
      "episode:  155  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  156  loss:  tensor([0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  157  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  158  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  159  loss:  tensor([-0.0117], grad_fn=<DivBackward0>)\n",
      "episode:  160  loss:  tensor([-0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  161  loss:  tensor([-1.2362e-07], grad_fn=<DivBackward0>)\n",
      "episode:  162  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  163  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  164  loss:  tensor([0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  165  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  166  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  167  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  168  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([60])) that is different to the input size (torch.Size([60, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  169  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  170  loss:  tensor([0.0126], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([62])) that is different to the input size (torch.Size([62, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  171  loss:  tensor([-8.1716e-08], grad_fn=<DivBackward0>)\n",
      "episode:  172  loss:  tensor([6.4133e-05], grad_fn=<DivBackward0>)\n",
      "episode:  173  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  174  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  175  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  176  loss:  tensor([-4.4785e-05], grad_fn=<DivBackward0>)\n",
      "episode:  177  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  178  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  179  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  180  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  181  loss:  tensor([2.3842e-08], grad_fn=<DivBackward0>)\n",
      "episode:  182  loss:  tensor([-5.9705e-05], grad_fn=<DivBackward0>)\n",
      "episode:  183  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([61])) that is different to the input size (torch.Size([61, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  184  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  185  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  186  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  187  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  188  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  189  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  190  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  191  loss:  tensor([-1.1462e-08], grad_fn=<DivBackward0>)\n",
      "episode:  192  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  193  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  194  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([77])) that is different to the input size (torch.Size([77, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  195  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  196  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  197  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  198  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  199  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  200  loss:  tensor([0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  201  loss:  tensor([5.7312e-08], grad_fn=<DivBackward0>)\n",
      "episode:  202  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  203  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  204  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  205  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  206  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  207  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  208  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  209  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  210  loss:  tensor([-0.0053], grad_fn=<DivBackward0>)\n",
      "episode:  211  loss:  tensor([4.9873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  212  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  213  loss:  tensor([7.5385e-05], grad_fn=<DivBackward0>)\n",
      "episode:  214  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  215  loss:  tensor([0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  216  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  217  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  218  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  219  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  220  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  221  loss:  tensor([-1.3862e-09], grad_fn=<DivBackward0>)\n",
      "episode:  222  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  223  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([79])) that is different to the input size (torch.Size([79, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  224  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  225  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  226  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  227  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  228  loss:  tensor([-0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  229  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  230  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  231  loss:  tensor([7.1526e-08], grad_fn=<DivBackward0>)\n",
      "episode:  232  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  233  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  234  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  235  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  236  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  237  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  238  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  239  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  240  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  241  loss:  tensor([-6.9539e-08], grad_fn=<DivBackward0>)\n",
      "episode:  242  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  243  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  244  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  245  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  246  loss:  tensor([-0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  247  loss:  tensor([-0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  248  loss:  tensor([0.0182], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([65])) that is different to the input size (torch.Size([65, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  249  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  250  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  251  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  252  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  253  loss:  tensor([-7.3522e-05], grad_fn=<DivBackward0>)\n",
      "episode:  254  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  255  loss:  tensor([-9.6445e-05], grad_fn=<DivBackward0>)\n",
      "episode:  256  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  257  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  258  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([118])) that is different to the input size (torch.Size([118, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  259  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  260  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  261  loss:  tensor([-1.8565e-08], grad_fn=<DivBackward0>)\n",
      "episode:  262  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  263  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  264  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  265  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  266  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  267  loss:  tensor([-0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  268  loss:  tensor([-0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  269  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  270  loss:  tensor([-0.0158], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([123])) that is different to the input size (torch.Size([123, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  271  loss:  tensor([-5.1367e-08], grad_fn=<DivBackward0>)\n",
      "episode:  272  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  273  loss:  tensor([-0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  274  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  275  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  276  loss:  tensor([0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  277  loss:  tensor([-0.0211], grad_fn=<DivBackward0>)\n",
      "episode:  278  loss:  tensor([-0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  279  loss:  tensor([-0.0309], grad_fn=<DivBackward0>)\n",
      "episode:  280  loss:  tensor([0.0071], grad_fn=<DivBackward0>)\n",
      "episode:  281  loss:  tensor([2.4020e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([68])) that is different to the input size (torch.Size([68, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  282  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  283  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  284  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  285  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  286  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  287  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  288  loss:  tensor([0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  289  loss:  tensor([0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  290  loss:  tensor([0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  291  loss:  tensor([5.6832e-08], grad_fn=<DivBackward0>)\n",
      "episode:  292  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  293  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  294  loss:  tensor([0.0053], grad_fn=<DivBackward0>)\n",
      "episode:  295  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  296  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  297  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  298  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  299  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  300  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  301  loss:  tensor([-3.8691e-08], grad_fn=<DivBackward0>)\n",
      "episode:  302  loss:  tensor([-6.2900e-05], grad_fn=<DivBackward0>)\n",
      "episode:  303  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  304  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  305  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  306  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  307  loss:  tensor([-0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  308  loss:  tensor([-0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  309  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  310  loss:  tensor([-0.0126], grad_fn=<DivBackward0>)\n",
      "episode:  311  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  312  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  313  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  314  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  315  loss:  tensor([0.0142], grad_fn=<DivBackward0>)\n",
      "episode:  316  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  317  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  318  loss:  tensor([0.0097], grad_fn=<DivBackward0>)\n",
      "episode:  319  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  320  loss:  tensor([0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  321  loss:  tensor([-4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  322  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  323  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  324  loss:  tensor([0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  325  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  326  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  327  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  328  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  329  loss:  tensor([0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  330  loss:  tensor([-0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  331  loss:  tensor([2.7978e-08], grad_fn=<DivBackward0>)\n",
      "episode:  332  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  333  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  334  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  335  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  336  loss:  tensor([-2.8817e-05], grad_fn=<DivBackward0>)\n",
      "episode:  337  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  338  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  339  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  340  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  341  loss:  tensor([5.1260e-08], grad_fn=<DivBackward0>)\n",
      "episode:  342  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  343  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  344  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  345  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([72])) that is different to the input size (torch.Size([72, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  346  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  347  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  348  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  349  loss:  tensor([-0.0048], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([85])) that is different to the input size (torch.Size([85, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  350  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  351  loss:  tensor([2.1458e-08], grad_fn=<DivBackward0>)\n",
      "episode:  352  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  353  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  354  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  355  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  356  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  357  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  358  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([75])) that is different to the input size (torch.Size([75, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  359  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  360  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  361  loss:  tensor([-6.4020e-08], grad_fn=<DivBackward0>)\n",
      "episode:  362  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  363  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  364  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([106])) that is different to the input size (torch.Size([106, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  365  loss:  tensor([4.6678e-05], grad_fn=<DivBackward0>)\n",
      "episode:  366  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  367  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  368  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  369  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  370  loss:  tensor([-4.2282e-05], grad_fn=<DivBackward0>)\n",
      "episode:  371  loss:  tensor([-1.2272e-07], grad_fn=<DivBackward0>)\n",
      "episode:  372  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  373  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  374  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  375  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  376  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  377  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  378  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  379  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  380  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  381  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  382  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  383  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  384  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  385  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  386  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  387  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  388  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  389  loss:  tensor([0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  390  loss:  tensor([1.4849e-05], grad_fn=<DivBackward0>)\n",
      "episode:  391  loss:  tensor([1.2772e-07], grad_fn=<DivBackward0>)\n",
      "episode:  392  loss:  tensor([-9.7080e-05], grad_fn=<DivBackward0>)\n",
      "episode:  393  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  394  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  395  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  396  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  397  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  398  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  399  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([71])) that is different to the input size (torch.Size([71, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  400  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  401  loss:  tensor([-1.1921e-07], grad_fn=<DivBackward0>)\n",
      "episode:  402  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  403  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  404  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  405  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  406  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  407  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  408  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  409  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  410  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  411  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  412  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  413  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  414  loss:  tensor([0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  415  loss:  tensor([-0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  416  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  417  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  418  loss:  tensor([-0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  419  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  420  loss:  tensor([-0.0145], grad_fn=<DivBackward0>)\n",
      "episode:  421  loss:  tensor([-8.7328e-08], grad_fn=<DivBackward0>)\n",
      "episode:  422  loss:  tensor([2.6985e-05], grad_fn=<DivBackward0>)\n",
      "episode:  423  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  424  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  425  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  426  loss:  tensor([0.0103], grad_fn=<DivBackward0>)\n",
      "episode:  427  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  428  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  429  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  430  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([94])) that is different to the input size (torch.Size([94, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  431  loss:  tensor([-5.0727e-08], grad_fn=<DivBackward0>)\n",
      "episode:  432  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  433  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  434  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  435  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  436  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  437  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  438  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  439  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  440  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([102])) that is different to the input size (torch.Size([102, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  441  loss:  tensor([5.8436e-09], grad_fn=<DivBackward0>)\n",
      "episode:  442  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  443  loss:  tensor([-0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  444  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  445  loss:  tensor([0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  446  loss:  tensor([0.0094], grad_fn=<DivBackward0>)\n",
      "episode:  447  loss:  tensor([0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  448  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  449  loss:  tensor([-0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  450  loss:  tensor([-0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  451  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  452  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  453  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  454  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([120])) that is different to the input size (torch.Size([120, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  455  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  456  loss:  tensor([-0.0091], grad_fn=<DivBackward0>)\n",
      "episode:  457  loss:  tensor([-0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  458  loss:  tensor([0.0088], grad_fn=<DivBackward0>)\n",
      "episode:  459  loss:  tensor([-0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  460  loss:  tensor([0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  461  loss:  tensor([-4.8328e-09], grad_fn=<DivBackward0>)\n",
      "episode:  462  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  463  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  464  loss:  tensor([-0.0118], grad_fn=<DivBackward0>)\n",
      "episode:  465  loss:  tensor([-0.0180], grad_fn=<DivBackward0>)\n",
      "episode:  466  loss:  tensor([-0.0096], grad_fn=<DivBackward0>)\n",
      "episode:  467  loss:  tensor([-0.0386], grad_fn=<DivBackward0>)\n",
      "episode:  468  loss:  tensor([-0.0133], grad_fn=<DivBackward0>)\n",
      "episode:  469  loss:  tensor([-0.0381], grad_fn=<DivBackward0>)\n",
      "episode:  470  loss:  tensor([-0.0890], grad_fn=<DivBackward0>)\n",
      "episode:  471  loss:  tensor([-6.3862e-09], grad_fn=<DivBackward0>)\n",
      "episode:  472  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  473  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  474  loss:  tensor([-0.0501], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  475  loss:  tensor([0.1255], grad_fn=<DivBackward0>)\n",
      "episode:  476  loss:  tensor([-0.0372], grad_fn=<DivBackward0>)\n",
      "episode:  477  loss:  tensor([-0.0861], grad_fn=<DivBackward0>)\n",
      "episode:  478  loss:  tensor([-0.0624], grad_fn=<DivBackward0>)\n",
      "episode:  479  loss:  tensor([-0.0122], grad_fn=<DivBackward0>)\n",
      "episode:  480  loss:  tensor([-0.0131], grad_fn=<DivBackward0>)\n",
      "episode:  481  loss:  tensor([-3.7493e-08], grad_fn=<DivBackward0>)\n",
      "episode:  482  loss:  tensor([-9.5990e-05], grad_fn=<DivBackward0>)\n",
      "episode:  483  loss:  tensor([0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  484  loss:  tensor([-0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  485  loss:  tensor([-0.0187], grad_fn=<DivBackward0>)\n",
      "episode:  486  loss:  tensor([-0.0123], grad_fn=<DivBackward0>)\n",
      "episode:  487  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  488  loss:  tensor([0.0329], grad_fn=<DivBackward0>)\n",
      "episode:  489  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  490  loss:  tensor([-0.0080], grad_fn=<DivBackward0>)\n",
      "episode:  491  loss:  tensor([-8.5149e-09], grad_fn=<DivBackward0>)\n",
      "episode:  492  loss:  tensor([-0.0112], grad_fn=<DivBackward0>)\n",
      "episode:  493  loss:  tensor([-0.0166], grad_fn=<DivBackward0>)\n",
      "episode:  494  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  495  loss:  tensor([0.0315], grad_fn=<DivBackward0>)\n",
      "episode:  496  loss:  tensor([0.0384], grad_fn=<DivBackward0>)\n",
      "episode:  497  loss:  tensor([0.0231], grad_fn=<DivBackward0>)\n",
      "episode:  498  loss:  tensor([-0.0284], grad_fn=<DivBackward0>)\n",
      "episode:  499  loss:  tensor([0.0237], grad_fn=<DivBackward0>)\n",
      "episode:  500  loss:  tensor([0.0360], grad_fn=<DivBackward0>)\n",
      "episode:  501  loss:  tensor([5.2592e-08], grad_fn=<DivBackward0>)\n",
      "episode:  502  loss:  tensor([-0.0253], grad_fn=<DivBackward0>)\n",
      "episode:  503  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  504  loss:  tensor([0.0073], grad_fn=<DivBackward0>)\n",
      "episode:  505  loss:  tensor([-0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  506  loss:  tensor([0.0087], grad_fn=<DivBackward0>)\n",
      "episode:  507  loss:  tensor([0.0118], grad_fn=<DivBackward0>)\n",
      "episode:  508  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  509  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  510  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  511  loss:  tensor([5.2154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  512  loss:  tensor([-0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  513  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  514  loss:  tensor([-0.0131], grad_fn=<DivBackward0>)\n",
      "episode:  515  loss:  tensor([-0.0084], grad_fn=<DivBackward0>)\n",
      "episode:  516  loss:  tensor([-0.0185], grad_fn=<DivBackward0>)\n",
      "episode:  517  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  518  loss:  tensor([-0.0188], grad_fn=<DivBackward0>)\n",
      "episode:  519  loss:  tensor([-0.0144], grad_fn=<DivBackward0>)\n",
      "episode:  520  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  521  loss:  tensor([9.5367e-09], grad_fn=<DivBackward0>)\n",
      "episode:  522  loss:  tensor([-6.2923e-05], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  523  loss:  tensor([0.0098], grad_fn=<DivBackward0>)\n",
      "episode:  524  loss:  tensor([0.0149], grad_fn=<DivBackward0>)\n",
      "episode:  525  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  526  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  527  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  528  loss:  tensor([0.0101], grad_fn=<DivBackward0>)\n",
      "episode:  529  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  530  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  531  loss:  tensor([-4.8429e-08], grad_fn=<DivBackward0>)\n",
      "episode:  532  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  533  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  534  loss:  tensor([0.0125], grad_fn=<DivBackward0>)\n",
      "episode:  535  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  536  loss:  tensor([0.0265], grad_fn=<DivBackward0>)\n",
      "episode:  537  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  538  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  539  loss:  tensor([-0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  540  loss:  tensor([-0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  541  loss:  tensor([7.7667e-08], grad_fn=<DivBackward0>)\n",
      "episode:  542  loss:  tensor([1.9676e-05], grad_fn=<DivBackward0>)\n",
      "episode:  543  loss:  tensor([0.0118], grad_fn=<DivBackward0>)\n",
      "episode:  544  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  545  loss:  tensor([-0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  546  loss:  tensor([-0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  547  loss:  tensor([0.0120], grad_fn=<DivBackward0>)\n",
      "episode:  548  loss:  tensor([-0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  549  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  550  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  551  loss:  tensor([-3.9051e-08], grad_fn=<DivBackward0>)\n",
      "episode:  552  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  553  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  554  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  555  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  556  loss:  tensor([0.0059], grad_fn=<DivBackward0>)\n",
      "episode:  557  loss:  tensor([-0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  558  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  559  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  560  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  561  loss:  tensor([-4.2575e-09], grad_fn=<DivBackward0>)\n",
      "episode:  562  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  563  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  564  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  565  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  566  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  567  loss:  tensor([-0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  568  loss:  tensor([-0.0087], grad_fn=<DivBackward0>)\n",
      "episode:  569  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  570  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  571  loss:  tensor([4.0978e-08], grad_fn=<DivBackward0>)\n",
      "episode:  572  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  573  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  574  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  575  loss:  tensor([0.0128], grad_fn=<DivBackward0>)\n",
      "episode:  576  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  577  loss:  tensor([0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  578  loss:  tensor([-0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  579  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  580  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  581  loss:  tensor([2.2076e-09], grad_fn=<DivBackward0>)\n",
      "episode:  582  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  583  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  584  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  585  loss:  tensor([-0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  586  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  587  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  588  loss:  tensor([-0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  589  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  590  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  591  loss:  tensor([-3.0830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  592  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  593  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  594  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  595  loss:  tensor([-0.0114], grad_fn=<DivBackward0>)\n",
      "episode:  596  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  597  loss:  tensor([0.0407], grad_fn=<DivBackward0>)\n",
      "episode:  598  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  599  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  600  loss:  tensor([0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  601  loss:  tensor([1.2332e-08], grad_fn=<DivBackward0>)\n",
      "episode:  602  loss:  tensor([-0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  603  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  604  loss:  tensor([0.0429], grad_fn=<DivBackward0>)\n",
      "episode:  605  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  606  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  607  loss:  tensor([0.0069], grad_fn=<DivBackward0>)\n",
      "episode:  608  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  609  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  610  loss:  tensor([0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  611  loss:  tensor([2.2609e-08], grad_fn=<DivBackward0>)\n",
      "episode:  612  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  613  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  614  loss:  tensor([-0.0056], grad_fn=<DivBackward0>)\n",
      "episode:  615  loss:  tensor([0.0140], grad_fn=<DivBackward0>)\n",
      "episode:  616  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  617  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  618  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  619  loss:  tensor([0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  620  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  621  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  622  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  623  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  624  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  625  loss:  tensor([0.0080], grad_fn=<DivBackward0>)\n",
      "episode:  626  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  627  loss:  tensor([-0.0054], grad_fn=<DivBackward0>)\n",
      "episode:  628  loss:  tensor([0.0118], grad_fn=<DivBackward0>)\n",
      "episode:  629  loss:  tensor([0.0126], grad_fn=<DivBackward0>)\n",
      "episode:  630  loss:  tensor([-0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  631  loss:  tensor([-2.3842e-08], grad_fn=<DivBackward0>)\n",
      "episode:  632  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  633  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  634  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  635  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  636  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  637  loss:  tensor([0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  638  loss:  tensor([0.0126], grad_fn=<DivBackward0>)\n",
      "episode:  639  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  640  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  641  loss:  tensor([-1.7030e-08], grad_fn=<DivBackward0>)\n",
      "episode:  642  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  643  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  644  loss:  tensor([0.0162], grad_fn=<DivBackward0>)\n",
      "episode:  645  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  646  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  647  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  648  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  649  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  650  loss:  tensor([0.0088], grad_fn=<DivBackward0>)\n",
      "episode:  651  loss:  tensor([4.4152e-09], grad_fn=<DivBackward0>)\n",
      "episode:  652  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  653  loss:  tensor([-0.0084], grad_fn=<DivBackward0>)\n",
      "episode:  654  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  655  loss:  tensor([0.0128], grad_fn=<DivBackward0>)\n",
      "episode:  656  loss:  tensor([0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  657  loss:  tensor([0.0077], grad_fn=<DivBackward0>)\n",
      "episode:  658  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  659  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  660  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  661  loss:  tensor([-5.7682e-09], grad_fn=<DivBackward0>)\n",
      "episode:  662  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  663  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  664  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  665  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  666  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  667  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  668  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  669  loss:  tensor([0.0043], grad_fn=<DivBackward0>)\n",
      "episode:  670  loss:  tensor([0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  671  loss:  tensor([1.0277e-08], grad_fn=<DivBackward0>)\n",
      "episode:  672  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  673  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  674  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  675  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  676  loss:  tensor([-0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  677  loss:  tensor([0.0283], grad_fn=<DivBackward0>)\n",
      "episode:  678  loss:  tensor([0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  679  loss:  tensor([0.0250], grad_fn=<DivBackward0>)\n",
      "episode:  680  loss:  tensor([-0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  681  loss:  tensor([8.5149e-08], grad_fn=<DivBackward0>)\n",
      "episode:  682  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  683  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  684  loss:  tensor([-6.9866e-05], grad_fn=<DivBackward0>)\n",
      "episode:  685  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  686  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  687  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  688  loss:  tensor([-0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  689  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  690  loss:  tensor([0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  691  loss:  tensor([1.6689e-08], grad_fn=<DivBackward0>)\n",
      "episode:  692  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  693  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  694  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  695  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  696  loss:  tensor([-0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  697  loss:  tensor([-0.0056], grad_fn=<DivBackward0>)\n",
      "episode:  698  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  699  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  700  loss:  tensor([-0.0076], grad_fn=<DivBackward0>)\n",
      "episode:  701  loss:  tensor([9.8656e-08], grad_fn=<DivBackward0>)\n",
      "episode:  702  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  703  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  704  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  705  loss:  tensor([0.0206], grad_fn=<DivBackward0>)\n",
      "episode:  706  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  707  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  708  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  709  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  710  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  711  loss:  tensor([8.8303e-09], grad_fn=<DivBackward0>)\n",
      "episode:  712  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  713  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  714  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  715  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  716  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  717  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  718  loss:  tensor([-0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  719  loss:  tensor([0.0237], grad_fn=<DivBackward0>)\n",
      "episode:  720  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  721  loss:  tensor([-8.0158e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  722  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  723  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  724  loss:  tensor([-2.1790e-05], grad_fn=<DivBackward0>)\n",
      "episode:  725  loss:  tensor([-0.0134], grad_fn=<DivBackward0>)\n",
      "episode:  726  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  727  loss:  tensor([0.0108], grad_fn=<DivBackward0>)\n",
      "episode:  728  loss:  tensor([0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  729  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  730  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  731  loss:  tensor([-1.7881e-08], grad_fn=<DivBackward0>)\n",
      "episode:  732  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  733  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  734  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  735  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  736  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  737  loss:  tensor([-0.0059], grad_fn=<DivBackward0>)\n",
      "episode:  738  loss:  tensor([0.0702], grad_fn=<DivBackward0>)\n",
      "episode:  739  loss:  tensor([0.1040], grad_fn=<DivBackward0>)\n",
      "episode:  740  loss:  tensor([0.0417], grad_fn=<DivBackward0>)\n",
      "episode:  741  loss:  tensor([-4.2841e-08], grad_fn=<DivBackward0>)\n",
      "episode:  742  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  743  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  744  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  745  loss:  tensor([0.0125], grad_fn=<DivBackward0>)\n",
      "episode:  746  loss:  tensor([0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  747  loss:  tensor([0.0113], grad_fn=<DivBackward0>)\n",
      "episode:  748  loss:  tensor([0.0075], grad_fn=<DivBackward0>)\n",
      "episode:  749  loss:  tensor([0.0073], grad_fn=<DivBackward0>)\n",
      "episode:  750  loss:  tensor([0.0110], grad_fn=<DivBackward0>)\n",
      "episode:  751  loss:  tensor([-4.2915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  752  loss:  tensor([0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  753  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  754  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  755  loss:  tensor([0.0474], grad_fn=<DivBackward0>)\n",
      "episode:  756  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  757  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  758  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  759  loss:  tensor([0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  760  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  761  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  762  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  763  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  764  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  765  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  766  loss:  tensor([-0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  767  loss:  tensor([0.0373], grad_fn=<DivBackward0>)\n",
      "episode:  768  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  769  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  770  loss:  tensor([-0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  771  loss:  tensor([-7.9473e-08], grad_fn=<DivBackward0>)\n",
      "episode:  772  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  773  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  774  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  775  loss:  tensor([9.6358e-05], grad_fn=<DivBackward0>)\n",
      "episode:  776  loss:  tensor([-2.1160e-06], grad_fn=<DivBackward0>)\n",
      "episode:  777  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  778  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  779  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  780  loss:  tensor([0.0130], grad_fn=<DivBackward0>)\n",
      "episode:  781  loss:  tensor([1.3245e-08], grad_fn=<DivBackward0>)\n",
      "episode:  782  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  783  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  784  loss:  tensor([0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  785  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  786  loss:  tensor([0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  787  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  788  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  789  loss:  tensor([-0.0112], grad_fn=<DivBackward0>)\n",
      "episode:  790  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  791  loss:  tensor([-3.3114e-08], grad_fn=<DivBackward0>)\n",
      "episode:  792  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  793  loss:  tensor([0.0083], grad_fn=<DivBackward0>)\n",
      "episode:  794  loss:  tensor([-0.0191], grad_fn=<DivBackward0>)\n",
      "episode:  795  loss:  tensor([0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  796  loss:  tensor([0.0128], grad_fn=<DivBackward0>)\n",
      "episode:  797  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  798  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  799  loss:  tensor([-0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  800  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  801  loss:  tensor([-9.1699e-09], grad_fn=<DivBackward0>)\n",
      "episode:  802  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  803  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  804  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  805  loss:  tensor([-0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  806  loss:  tensor([0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  807  loss:  tensor([0.0127], grad_fn=<DivBackward0>)\n",
      "episode:  808  loss:  tensor([0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  809  loss:  tensor([0.0202], grad_fn=<DivBackward0>)\n",
      "episode:  810  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  811  loss:  tensor([-3.5321e-08], grad_fn=<DivBackward0>)\n",
      "episode:  812  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  813  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  814  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  815  loss:  tensor([-0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  816  loss:  tensor([-0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  817  loss:  tensor([0.0246], grad_fn=<DivBackward0>)\n",
      "episode:  818  loss:  tensor([-0.0094], grad_fn=<DivBackward0>)\n",
      "episode:  819  loss:  tensor([-0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  820  loss:  tensor([0.0914], grad_fn=<DivBackward0>)\n",
      "episode:  821  loss:  tensor([-5.2452e-08], grad_fn=<DivBackward0>)\n",
      "episode:  822  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  823  loss:  tensor([0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  824  loss:  tensor([0.0116], grad_fn=<DivBackward0>)\n",
      "episode:  825  loss:  tensor([0.0127], grad_fn=<DivBackward0>)\n",
      "episode:  826  loss:  tensor([0.0092], grad_fn=<DivBackward0>)\n",
      "episode:  827  loss:  tensor([0.0101], grad_fn=<DivBackward0>)\n",
      "episode:  828  loss:  tensor([0.0165], grad_fn=<DivBackward0>)\n",
      "episode:  829  loss:  tensor([0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  830  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  831  loss:  tensor([5.2982e-08], grad_fn=<DivBackward0>)\n",
      "episode:  832  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  833  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  834  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  835  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  836  loss:  tensor([0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  837  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  838  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  839  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  840  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  841  loss:  tensor([-3.3379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  842  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  843  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  844  loss:  tensor([-0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  845  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  846  loss:  tensor([0.0082], grad_fn=<DivBackward0>)\n",
      "episode:  847  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  848  loss:  tensor([0.0077], grad_fn=<DivBackward0>)\n",
      "episode:  849  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  850  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  851  loss:  tensor([4.6359e-08], grad_fn=<DivBackward0>)\n",
      "episode:  852  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  853  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  854  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  855  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  856  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  857  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  858  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  859  loss:  tensor([0.0182], grad_fn=<DivBackward0>)\n",
      "episode:  860  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  861  loss:  tensor([9.4545e-08], grad_fn=<DivBackward0>)\n",
      "episode:  862  loss:  tensor([0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  863  loss:  tensor([-0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  864  loss:  tensor([0.0119], grad_fn=<DivBackward0>)\n",
      "episode:  865  loss:  tensor([0.0126], grad_fn=<DivBackward0>)\n",
      "episode:  866  loss:  tensor([0.0109], grad_fn=<DivBackward0>)\n",
      "episode:  867  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  868  loss:  tensor([0.0080], grad_fn=<DivBackward0>)\n",
      "episode:  869  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  870  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  871  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  872  loss:  tensor([-0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  873  loss:  tensor([0.0091], grad_fn=<DivBackward0>)\n",
      "episode:  874  loss:  tensor([-0.0088], grad_fn=<DivBackward0>)\n",
      "episode:  875  loss:  tensor([0.0197], grad_fn=<DivBackward0>)\n",
      "episode:  876  loss:  tensor([0.0339], grad_fn=<DivBackward0>)\n",
      "episode:  877  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  878  loss:  tensor([0.0478], grad_fn=<DivBackward0>)\n",
      "episode:  879  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  880  loss:  tensor([0.0396], grad_fn=<DivBackward0>)\n",
      "episode:  881  loss:  tensor([-4.8142e-08], grad_fn=<DivBackward0>)\n",
      "episode:  882  loss:  tensor([0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  883  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  884  loss:  tensor([-0.0097], grad_fn=<DivBackward0>)\n",
      "episode:  885  loss:  tensor([0.0197], grad_fn=<DivBackward0>)\n",
      "episode:  886  loss:  tensor([0.0206], grad_fn=<DivBackward0>)\n",
      "episode:  887  loss:  tensor([0.0163], grad_fn=<DivBackward0>)\n",
      "episode:  888  loss:  tensor([0.0053], grad_fn=<DivBackward0>)\n",
      "episode:  889  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  890  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  891  loss:  tensor([-2.7510e-08], grad_fn=<DivBackward0>)\n",
      "episode:  892  loss:  tensor([0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  893  loss:  tensor([0.0325], grad_fn=<DivBackward0>)\n",
      "episode:  894  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  895  loss:  tensor([0.0499], grad_fn=<DivBackward0>)\n",
      "episode:  896  loss:  tensor([-0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  897  loss:  tensor([-7.7259e-05], grad_fn=<DivBackward0>)\n",
      "episode:  898  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  899  loss:  tensor([-0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  900  loss:  tensor([0.0069], grad_fn=<DivBackward0>)\n",
      "episode:  901  loss:  tensor([4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  902  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  903  loss:  tensor([9.8083e-05], grad_fn=<DivBackward0>)\n",
      "episode:  904  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  905  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  906  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  907  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  908  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  909  loss:  tensor([0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  910  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  911  loss:  tensor([-3.3379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  912  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  913  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  914  loss:  tensor([7.3881e-05], grad_fn=<DivBackward0>)\n",
      "episode:  915  loss:  tensor([0.0111], grad_fn=<DivBackward0>)\n",
      "episode:  916  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  917  loss:  tensor([-0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  918  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  919  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  920  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  921  loss:  tensor([-3.6680e-08], grad_fn=<DivBackward0>)\n",
      "episode:  922  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  923  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  924  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  925  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  926  loss:  tensor([-0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  927  loss:  tensor([-0.0076], grad_fn=<DivBackward0>)\n",
      "episode:  928  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  929  loss:  tensor([-0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  930  loss:  tensor([0.0160], grad_fn=<DivBackward0>)\n",
      "episode:  931  loss:  tensor([2.1458e-08], grad_fn=<DivBackward0>)\n",
      "episode:  932  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  933  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  934  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  935  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  936  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  937  loss:  tensor([-0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  938  loss:  tensor([0.0128], grad_fn=<DivBackward0>)\n",
      "episode:  939  loss:  tensor([0.0234], grad_fn=<DivBackward0>)\n",
      "episode:  940  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  941  loss:  tensor([6.1591e-08], grad_fn=<DivBackward0>)\n",
      "episode:  942  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  943  loss:  tensor([0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  944  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  945  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  946  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  947  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  948  loss:  tensor([-4.0538e-05], grad_fn=<DivBackward0>)\n",
      "episode:  949  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  950  loss:  tensor([0.0071], grad_fn=<DivBackward0>)\n",
      "episode:  951  loss:  tensor([-4.9328e-08], grad_fn=<DivBackward0>)\n",
      "episode:  952  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  953  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  954  loss:  tensor([-0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  955  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  956  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  957  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  958  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  959  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  960  loss:  tensor([0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  961  loss:  tensor([-2.3842e-08], grad_fn=<DivBackward0>)\n",
      "episode:  962  loss:  tensor([-5.7273e-05], grad_fn=<DivBackward0>)\n",
      "episode:  963  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  964  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  965  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  966  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  967  loss:  tensor([0.0217], grad_fn=<DivBackward0>)\n",
      "episode:  968  loss:  tensor([-5.6106e-05], grad_fn=<DivBackward0>)\n",
      "episode:  969  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  970  loss:  tensor([4.5404e-05], grad_fn=<DivBackward0>)\n",
      "episode:  971  loss:  tensor([-5.3439e-08], grad_fn=<DivBackward0>)\n",
      "episode:  972  loss:  tensor([8.7317e-05], grad_fn=<DivBackward0>)\n",
      "episode:  973  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  974  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  975  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  976  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  977  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  978  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  979  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  980  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  981  loss:  tensor([-5.3218e-08], grad_fn=<DivBackward0>)\n",
      "episode:  982  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  983  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  984  loss:  tensor([0.0059], grad_fn=<DivBackward0>)\n",
      "episode:  985  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  986  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  987  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  988  loss:  tensor([0.0187], grad_fn=<DivBackward0>)\n",
      "episode:  989  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  990  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  991  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  992  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  993  loss:  tensor([0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  994  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  995  loss:  tensor([0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  996  loss:  tensor([0.0071], grad_fn=<DivBackward0>)\n",
      "episode:  997  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  998  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  999  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1000  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1001  loss:  tensor([1.9073e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1002  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1003  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1004  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1005  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  1006  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1007  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1008  loss:  tensor([-0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  1009  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1010  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1011  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1012  loss:  tensor([-4.4562e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1013  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1014  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1015  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1016  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1017  loss:  tensor([-0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  1018  loss:  tensor([0.0053], grad_fn=<DivBackward0>)\n",
      "episode:  1019  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  1020  loss:  tensor([-0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  1021  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1022  loss:  tensor([7.2447e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1023  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1024  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1025  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1026  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1027  loss:  tensor([-0.0054], grad_fn=<DivBackward0>)\n",
      "episode:  1028  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1029  loss:  tensor([0.0089], grad_fn=<DivBackward0>)\n",
      "episode:  1030  loss:  tensor([6.4644e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1031  loss:  tensor([-1.9868e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1032  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1033  loss:  tensor([-7.4918e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1034  loss:  tensor([0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  1035  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1036  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1037  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1038  loss:  tensor([-0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  1039  loss:  tensor([-0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1040  loss:  tensor([0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  1041  loss:  tensor([-3.8147e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1042  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1043  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  1044  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1045  loss:  tensor([-0.0098], grad_fn=<DivBackward0>)\n",
      "episode:  1046  loss:  tensor([0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  1047  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1048  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  1049  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1050  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1051  loss:  tensor([4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1052  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1053  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1054  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1055  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1056  loss:  tensor([0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  1057  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1058  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1059  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  1060  loss:  tensor([-0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  1061  loss:  tensor([-3.3114e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1062  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1063  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1064  loss:  tensor([-0.0094], grad_fn=<DivBackward0>)\n",
      "episode:  1065  loss:  tensor([0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1066  loss:  tensor([0.0053], grad_fn=<DivBackward0>)\n",
      "episode:  1067  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1068  loss:  tensor([0.0140], grad_fn=<DivBackward0>)\n",
      "episode:  1069  loss:  tensor([0.0128], grad_fn=<DivBackward0>)\n",
      "episode:  1070  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  1071  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1072  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1073  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1074  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1075  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  1076  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1077  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1078  loss:  tensor([0.0106], grad_fn=<DivBackward0>)\n",
      "episode:  1079  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1080  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1081  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1082  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1083  loss:  tensor([4.0342e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1084  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1085  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1086  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1087  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1088  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1089  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1090  loss:  tensor([0.0252], grad_fn=<DivBackward0>)\n",
      "episode:  1091  loss:  tensor([-6.9881e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1092  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1093  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1094  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1095  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1096  loss:  tensor([0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  1097  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  1098  loss:  tensor([0.0087], grad_fn=<DivBackward0>)\n",
      "episode:  1099  loss:  tensor([0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  1100  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1101  loss:  tensor([5.4836e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1102  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1103  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  1104  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1105  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1106  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  1107  loss:  tensor([-0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  1108  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1109  loss:  tensor([0.0115], grad_fn=<DivBackward0>)\n",
      "episode:  1110  loss:  tensor([0.0736], grad_fn=<DivBackward0>)\n",
      "episode:  1111  loss:  tensor([-4.1265e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1112  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1113  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1114  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1115  loss:  tensor([-9.3127e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1116  loss:  tensor([0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  1117  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1118  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1119  loss:  tensor([-3.7668e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1120  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1121  loss:  tensor([2.8699e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1122  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1123  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1124  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1125  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1126  loss:  tensor([8.4376e-06], grad_fn=<DivBackward0>)\n",
      "episode:  1127  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1128  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1129  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1130  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1131  loss:  tensor([4.2575e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1132  loss:  tensor([-2.9082e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1133  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  1134  loss:  tensor([-2.0071e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1135  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1136  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1137  loss:  tensor([0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1138  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1139  loss:  tensor([-0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  1140  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1141  loss:  tensor([7.7265e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1142  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1143  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1144  loss:  tensor([-0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1145  loss:  tensor([0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1146  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1147  loss:  tensor([0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1148  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1149  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1150  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1151  loss:  tensor([-3.7529e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1152  loss:  tensor([1.8251e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1153  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1154  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1155  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1156  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1157  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1158  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1159  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1160  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1161  loss:  tensor([2.0632e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1162  loss:  tensor([-2.1880e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1163  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1164  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1165  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1166  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1167  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1168  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1169  loss:  tensor([-6.0995e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1170  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1171  loss:  tensor([-3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1172  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1173  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1174  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1175  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1176  loss:  tensor([0.0117], grad_fn=<DivBackward0>)\n",
      "episode:  1177  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1178  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1179  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  1180  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1181  loss:  tensor([-6.3862e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1182  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1183  loss:  tensor([0.0077], grad_fn=<DivBackward0>)\n",
      "episode:  1184  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1185  loss:  tensor([0.0075], grad_fn=<DivBackward0>)\n",
      "episode:  1186  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1187  loss:  tensor([0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  1188  loss:  tensor([0.0043], grad_fn=<DivBackward0>)\n",
      "episode:  1189  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1190  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1191  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1192  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1193  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1194  loss:  tensor([0.0059], grad_fn=<DivBackward0>)\n",
      "episode:  1195  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1196  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1197  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1198  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1199  loss:  tensor([-0.0077], grad_fn=<DivBackward0>)\n",
      "episode:  1200  loss:  tensor([0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  1201  loss:  tensor([-2.2076e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1202  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1203  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1204  loss:  tensor([0.0055], grad_fn=<DivBackward0>)\n",
      "episode:  1205  loss:  tensor([-0.0044], grad_fn=<DivBackward0>)\n",
      "episode:  1206  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1207  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1208  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  1209  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1210  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1211  loss:  tensor([-5.1090e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1212  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1213  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1214  loss:  tensor([0.0064], grad_fn=<DivBackward0>)\n",
      "episode:  1215  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1216  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1217  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1218  loss:  tensor([0.0125], grad_fn=<DivBackward0>)\n",
      "episode:  1219  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1220  loss:  tensor([-7.5262e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1221  loss:  tensor([-6.6227e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1222  loss:  tensor([8.2287e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1223  loss:  tensor([7.0767e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1224  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1225  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1226  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1227  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1228  loss:  tensor([0.0089], grad_fn=<DivBackward0>)\n",
      "episode:  1229  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1230  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1231  loss:  tensor([-8.8303e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1232  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1233  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1234  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1235  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1236  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1237  loss:  tensor([-0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  1238  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1239  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1240  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1241  loss:  tensor([9.9341e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1242  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1243  loss:  tensor([7.2465e-06], grad_fn=<DivBackward0>)\n",
      "episode:  1244  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1245  loss:  tensor([3.4653e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1246  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1247  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1248  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  1249  loss:  tensor([0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  1250  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1251  loss:  tensor([3.8317e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1252  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1253  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1254  loss:  tensor([8.8625e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1255  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1256  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1257  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1258  loss:  tensor([0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1259  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1260  loss:  tensor([0.0248], grad_fn=<DivBackward0>)\n",
      "episode:  1261  loss:  tensor([5.0774e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1262  loss:  tensor([1.6789e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1263  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1264  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1265  loss:  tensor([-0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  1266  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1267  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1268  loss:  tensor([-0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  1269  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1270  loss:  tensor([0.0084], grad_fn=<DivBackward0>)\n",
      "episode:  1271  loss:  tensor([-4.6359e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1272  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1273  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1274  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1275  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1276  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1277  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1278  loss:  tensor([-0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  1279  loss:  tensor([-0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  1280  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1281  loss:  tensor([-4.5850e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1282  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1283  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1284  loss:  tensor([0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1285  loss:  tensor([-0.0157], grad_fn=<DivBackward0>)\n",
      "episode:  1286  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1287  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  1288  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1289  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1290  loss:  tensor([0.0354], grad_fn=<DivBackward0>)\n",
      "episode:  1291  loss:  tensor([-4.8567e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1292  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1293  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1294  loss:  tensor([0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  1295  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1296  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1297  loss:  tensor([0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  1298  loss:  tensor([0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  1299  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1300  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1301  loss:  tensor([2.8049e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1302  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1303  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1304  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1305  loss:  tensor([0.0058], grad_fn=<DivBackward0>)\n",
      "episode:  1306  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1307  loss:  tensor([0.0079], grad_fn=<DivBackward0>)\n",
      "episode:  1308  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1309  loss:  tensor([-4.0891e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1310  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1311  loss:  tensor([5.2727e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1312  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1313  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1314  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1315  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1316  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1317  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1318  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1319  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1320  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1321  loss:  tensor([4.1107e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1322  loss:  tensor([-0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1323  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1324  loss:  tensor([-0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  1325  loss:  tensor([0.0036], grad_fn=<DivBackward0>)\n",
      "episode:  1326  loss:  tensor([0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  1327  loss:  tensor([-0.0138], grad_fn=<DivBackward0>)\n",
      "episode:  1328  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  1329  loss:  tensor([0.0068], grad_fn=<DivBackward0>)\n",
      "episode:  1330  loss:  tensor([-0.0123], grad_fn=<DivBackward0>)\n",
      "episode:  1331  loss:  tensor([-8.8303e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1332  loss:  tensor([2.4368e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1333  loss:  tensor([3.5777e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1334  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1335  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1336  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1337  loss:  tensor([-8.8695e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1338  loss:  tensor([0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  1339  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1340  loss:  tensor([0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  1341  loss:  tensor([-1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1342  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1343  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1344  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1345  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1346  loss:  tensor([-0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1347  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  1348  loss:  tensor([-0.0056], grad_fn=<DivBackward0>)\n",
      "episode:  1349  loss:  tensor([-0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1350  loss:  tensor([-0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1351  loss:  tensor([-8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1352  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1353  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1354  loss:  tensor([0.0079], grad_fn=<DivBackward0>)\n",
      "episode:  1355  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  1356  loss:  tensor([0.0059], grad_fn=<DivBackward0>)\n",
      "episode:  1357  loss:  tensor([0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  1358  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1359  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1360  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1361  loss:  tensor([3.3114e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1362  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1363  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1364  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1365  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1366  loss:  tensor([9.2628e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1367  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1368  loss:  tensor([-0.0092], grad_fn=<DivBackward0>)\n",
      "episode:  1369  loss:  tensor([-0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1370  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1371  loss:  tensor([1.8626e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1372  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1373  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1374  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1375  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1376  loss:  tensor([0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1377  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1378  loss:  tensor([-0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1379  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1380  loss:  tensor([0.0066], grad_fn=<DivBackward0>)\n",
      "episode:  1381  loss:  tensor([-2.0553e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1382  loss:  tensor([8.5855e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1383  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1384  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1385  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1386  loss:  tensor([-0.0043], grad_fn=<DivBackward0>)\n",
      "episode:  1387  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1388  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1389  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1390  loss:  tensor([0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1391  loss:  tensor([-5.0435e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1392  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1393  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1394  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1395  loss:  tensor([-0.0050], grad_fn=<DivBackward0>)\n",
      "episode:  1396  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1397  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1398  loss:  tensor([0.0171], grad_fn=<DivBackward0>)\n",
      "episode:  1399  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1400  loss:  tensor([-6.2989e-06], grad_fn=<DivBackward0>)\n",
      "episode:  1401  loss:  tensor([-2.8775e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1402  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1403  loss:  tensor([-1.9964e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1404  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1405  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1406  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1407  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1408  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  1409  loss:  tensor([-0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1410  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1411  loss:  tensor([7.6635e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1412  loss:  tensor([4.7037e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1413  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1414  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1415  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1416  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1417  loss:  tensor([-0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  1418  loss:  tensor([0.0143], grad_fn=<DivBackward0>)\n",
      "episode:  1419  loss:  tensor([0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  1420  loss:  tensor([0.0082], grad_fn=<DivBackward0>)\n",
      "episode:  1421  loss:  tensor([-9.0310e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1422  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1423  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1424  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1425  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1426  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1427  loss:  tensor([-0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  1428  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  1429  loss:  tensor([-0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  1430  loss:  tensor([-0.0082], grad_fn=<DivBackward0>)\n",
      "episode:  1431  loss:  tensor([2.8610e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1432  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1433  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1434  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1435  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1436  loss:  tensor([0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1437  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1438  loss:  tensor([0.0143], grad_fn=<DivBackward0>)\n",
      "episode:  1439  loss:  tensor([0.0065], grad_fn=<DivBackward0>)\n",
      "episode:  1440  loss:  tensor([-0.0085], grad_fn=<DivBackward0>)\n",
      "episode:  1441  loss:  tensor([6.1660e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1442  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1443  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1444  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1445  loss:  tensor([0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1446  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1447  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1448  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1449  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1450  loss:  tensor([-0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  1451  loss:  tensor([3.3776e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1452  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1453  loss:  tensor([-7.0694e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1454  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1455  loss:  tensor([2.0405e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1456  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1457  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1458  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1459  loss:  tensor([-0.0019], grad_fn=<DivBackward0>)\n",
      "episode:  1460  loss:  tensor([-0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1461  loss:  tensor([3.5390e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1462  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1463  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1464  loss:  tensor([0.0049], grad_fn=<DivBackward0>)\n",
      "episode:  1465  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1466  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1467  loss:  tensor([0.0032], grad_fn=<DivBackward0>)\n",
      "episode:  1468  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1469  loss:  tensor([0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1470  loss:  tensor([0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1471  loss:  tensor([-2.1855e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1472  loss:  tensor([1.6224e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1473  loss:  tensor([-9.6241e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1474  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1475  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1476  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1477  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1478  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1479  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1480  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1481  loss:  tensor([-4.0446e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1482  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1483  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1484  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1485  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1486  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1487  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1488  loss:  tensor([-0.0086], grad_fn=<DivBackward0>)\n",
      "episode:  1489  loss:  tensor([0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1490  loss:  tensor([-0.0040], grad_fn=<DivBackward0>)\n",
      "episode:  1491  loss:  tensor([-1.1038e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1492  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1493  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1494  loss:  tensor([0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1495  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1496  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  1497  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1498  loss:  tensor([0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1499  loss:  tensor([0.0110], grad_fn=<DivBackward0>)\n",
      "episode:  1500  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1501  loss:  tensor([1.6443e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1502  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1503  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1504  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1505  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1506  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1507  loss:  tensor([-0.0054], grad_fn=<DivBackward0>)\n",
      "episode:  1508  loss:  tensor([-0.0084], grad_fn=<DivBackward0>)\n",
      "episode:  1509  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1510  loss:  tensor([-0.0067], grad_fn=<DivBackward0>)\n",
      "episode:  1511  loss:  tensor([5.5759e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1512  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1513  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1514  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1515  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1516  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1517  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1518  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1519  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1520  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1521  loss:  tensor([2.2925e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1522  loss:  tensor([-5.8363e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1523  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1524  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1525  loss:  tensor([0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1526  loss:  tensor([0.0060], grad_fn=<DivBackward0>)\n",
      "episode:  1527  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1528  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1529  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1530  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1531  loss:  tensor([4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1532  loss:  tensor([-6.4327e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1533  loss:  tensor([4.2387e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1534  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1535  loss:  tensor([1.4691e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1536  loss:  tensor([-0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1537  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1538  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1539  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1540  loss:  tensor([0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  1541  loss:  tensor([1.8498e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1542  loss:  tensor([-0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  1543  loss:  tensor([-0.0034], grad_fn=<DivBackward0>)\n",
      "episode:  1544  loss:  tensor([-0.0056], grad_fn=<DivBackward0>)\n",
      "episode:  1545  loss:  tensor([-0.0089], grad_fn=<DivBackward0>)\n",
      "episode:  1546  loss:  tensor([0.0039], grad_fn=<DivBackward0>)\n",
      "episode:  1547  loss:  tensor([0.0115], grad_fn=<DivBackward0>)\n",
      "episode:  1548  loss:  tensor([-0.0188], grad_fn=<DivBackward0>)\n",
      "episode:  1549  loss:  tensor([-0.0025], grad_fn=<DivBackward0>)\n",
      "episode:  1550  loss:  tensor([0.0079], grad_fn=<DivBackward0>)\n",
      "episode:  1551  loss:  tensor([6.3862e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1552  loss:  tensor([-9.7930e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1553  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1554  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1555  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1556  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1557  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1558  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1559  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1560  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1561  loss:  tensor([7.1526e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1562  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1563  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1564  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1565  loss:  tensor([0.0085], grad_fn=<DivBackward0>)\n",
      "episode:  1566  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1567  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1568  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1569  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1570  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1571  loss:  tensor([7.0643e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1572  loss:  tensor([-9.2610e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1573  loss:  tensor([-7.9864e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1574  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1575  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1576  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1577  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1578  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1579  loss:  tensor([-0.0029], grad_fn=<DivBackward0>)\n",
      "episode:  1580  loss:  tensor([-0.0077], grad_fn=<DivBackward0>)\n",
      "episode:  1581  loss:  tensor([-3.8317e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1582  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1583  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1584  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1585  loss:  tensor([-0.0070], grad_fn=<DivBackward0>)\n",
      "episode:  1586  loss:  tensor([0.0167], grad_fn=<DivBackward0>)\n",
      "episode:  1587  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1588  loss:  tensor([-0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1589  loss:  tensor([0.0081], grad_fn=<DivBackward0>)\n",
      "episode:  1590  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1591  loss:  tensor([-2.3416e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1592  loss:  tensor([3.2825e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1593  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1594  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1595  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1596  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1597  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1598  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1599  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1600  loss:  tensor([-0.0035], grad_fn=<DivBackward0>)\n",
      "episode:  1601  loss:  tensor([-8.3021e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1602  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1603  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1604  loss:  tensor([9.3685e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1605  loss:  tensor([2.5960e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1606  loss:  tensor([0.0100], grad_fn=<DivBackward0>)\n",
      "episode:  1607  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1608  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1609  loss:  tensor([0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1610  loss:  tensor([-2.5124e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1611  loss:  tensor([1.4387e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1612  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1613  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1614  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1615  loss:  tensor([9.9065e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1616  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1617  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1618  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1619  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1620  loss:  tensor([0.0098], grad_fn=<DivBackward0>)\n",
      "episode:  1621  loss:  tensor([4.7684e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1622  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1623  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1624  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1625  loss:  tensor([0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  1626  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1627  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1628  loss:  tensor([0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  1629  loss:  tensor([-0.0038], grad_fn=<DivBackward0>)\n",
      "episode:  1630  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1631  loss:  tensor([2.6491e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1632  loss:  tensor([2.1011e-06], grad_fn=<DivBackward0>)\n",
      "episode:  1633  loss:  tensor([-2.5760e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1634  loss:  tensor([-4.7445e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1635  loss:  tensor([-8.1553e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1636  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1637  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1638  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1639  loss:  tensor([0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1640  loss:  tensor([0.0110], grad_fn=<DivBackward0>)\n",
      "episode:  1641  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1642  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1643  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1644  loss:  tensor([0.0090], grad_fn=<DivBackward0>)\n",
      "episode:  1645  loss:  tensor([-0.0047], grad_fn=<DivBackward0>)\n",
      "episode:  1646  loss:  tensor([0.0022], grad_fn=<DivBackward0>)\n",
      "episode:  1647  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1648  loss:  tensor([0.0130], grad_fn=<DivBackward0>)\n",
      "episode:  1649  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  1650  loss:  tensor([-2.3356e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1651  loss:  tensor([3.7253e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1652  loss:  tensor([-7.6950e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1653  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1654  loss:  tensor([-8.5540e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1655  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1656  loss:  tensor([-0.0009], grad_fn=<DivBackward0>)\n",
      "episode:  1657  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1658  loss:  tensor([-0.0026], grad_fn=<DivBackward0>)\n",
      "episode:  1659  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1660  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1661  loss:  tensor([-5.4017e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1662  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1663  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1664  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1665  loss:  tensor([0.0136], grad_fn=<DivBackward0>)\n",
      "episode:  1666  loss:  tensor([-0.0024], grad_fn=<DivBackward0>)\n",
      "episode:  1667  loss:  tensor([-0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1668  loss:  tensor([-0.0028], grad_fn=<DivBackward0>)\n",
      "episode:  1669  loss:  tensor([0.0054], grad_fn=<DivBackward0>)\n",
      "episode:  1670  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1671  loss:  tensor([1.3245e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1672  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1673  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1674  loss:  tensor([4.2455e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1675  loss:  tensor([6.9492e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1676  loss:  tensor([-3.3530e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1677  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1678  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1679  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1680  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1681  loss:  tensor([-7.2377e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1682  loss:  tensor([-0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1683  loss:  tensor([3.5366e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1684  loss:  tensor([0.0137], grad_fn=<DivBackward0>)\n",
      "episode:  1685  loss:  tensor([0.0030], grad_fn=<DivBackward0>)\n",
      "episode:  1686  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1687  loss:  tensor([-1.6029e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1688  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1689  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1690  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1691  loss:  tensor([-4.7684e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1692  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1693  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1694  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1695  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1696  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1697  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1698  loss:  tensor([-0.0031], grad_fn=<DivBackward0>)\n",
      "episode:  1699  loss:  tensor([-0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1700  loss:  tensor([8.3497e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1701  loss:  tensor([3.2885e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1702  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1703  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1704  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1705  loss:  tensor([-0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1706  loss:  tensor([0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1707  loss:  tensor([0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1708  loss:  tensor([0.0107], grad_fn=<DivBackward0>)\n",
      "episode:  1709  loss:  tensor([-0.0115], grad_fn=<DivBackward0>)\n",
      "episode:  1710  loss:  tensor([-0.0057], grad_fn=<DivBackward0>)\n",
      "episode:  1711  loss:  tensor([2.3416e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1712  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1713  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1714  loss:  tensor([0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1715  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1716  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1717  loss:  tensor([0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1718  loss:  tensor([0.0085], grad_fn=<DivBackward0>)\n",
      "episode:  1719  loss:  tensor([-0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1720  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1721  loss:  tensor([-3.4387e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1722  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1723  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1724  loss:  tensor([0.0060], grad_fn=<DivBackward0>)\n",
      "episode:  1725  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1726  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1727  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1728  loss:  tensor([-2.4783e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1729  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1730  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1731  loss:  tensor([1.8062e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1732  loss:  tensor([-0.0008], grad_fn=<DivBackward0>)\n",
      "episode:  1733  loss:  tensor([-0.0109], grad_fn=<DivBackward0>)\n",
      "episode:  1734  loss:  tensor([-0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1735  loss:  tensor([-0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1736  loss:  tensor([-0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1737  loss:  tensor([0.0121], grad_fn=<DivBackward0>)\n",
      "episode:  1738  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1739  loss:  tensor([-0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1740  loss:  tensor([0.0156], grad_fn=<DivBackward0>)\n",
      "episode:  1741  loss:  tensor([2.1287e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1742  loss:  tensor([0.0014], grad_fn=<DivBackward0>)\n",
      "episode:  1743  loss:  tensor([0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1744  loss:  tensor([0.0015], grad_fn=<DivBackward0>)\n",
      "episode:  1745  loss:  tensor([0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1746  loss:  tensor([-0.0007], grad_fn=<DivBackward0>)\n",
      "episode:  1747  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1748  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1749  loss:  tensor([0.0042], grad_fn=<DivBackward0>)\n",
      "episode:  1750  loss:  tensor([-0.0005], grad_fn=<DivBackward0>)\n",
      "episode:  1751  loss:  tensor([1.5453e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1752  loss:  tensor([-6.0689e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1753  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1754  loss:  tensor([0.0110], grad_fn=<DivBackward0>)\n",
      "episode:  1755  loss:  tensor([0.0099], grad_fn=<DivBackward0>)\n",
      "episode:  1756  loss:  tensor([5.9927e-05], grad_fn=<DivBackward0>)\n",
      "episode:  1757  loss:  tensor([0.0020], grad_fn=<DivBackward0>)\n",
      "episode:  1758  loss:  tensor([0.0109], grad_fn=<DivBackward0>)\n",
      "episode:  1759  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1760  loss:  tensor([0.0134], grad_fn=<DivBackward0>)\n",
      "episode:  1761  loss:  tensor([-2.2925e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1762  loss:  tensor([0.0004], grad_fn=<DivBackward0>)\n",
      "episode:  1763  loss:  tensor([-0.0002], grad_fn=<DivBackward0>)\n",
      "episode:  1764  loss:  tensor([-0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1765  loss:  tensor([-0.0048], grad_fn=<DivBackward0>)\n",
      "episode:  1766  loss:  tensor([0.0076], grad_fn=<DivBackward0>)\n",
      "episode:  1767  loss:  tensor([0.0530], grad_fn=<DivBackward0>)\n",
      "episode:  1768  loss:  tensor([0.0011], grad_fn=<DivBackward0>)\n",
      "episode:  1769  loss:  tensor([0.0072], grad_fn=<DivBackward0>)\n",
      "episode:  1770  loss:  tensor([0.0093], grad_fn=<DivBackward0>)\n",
      "episode:  1771  loss:  tensor([2.2925e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1772  loss:  tensor([-0.0063], grad_fn=<DivBackward0>)\n",
      "episode:  1773  loss:  tensor([0.0037], grad_fn=<DivBackward0>)\n",
      "episode:  1774  loss:  tensor([0.0191], grad_fn=<DivBackward0>)\n",
      "episode:  1775  loss:  tensor([0.0155], grad_fn=<DivBackward0>)\n",
      "episode:  1776  loss:  tensor([0.0254], grad_fn=<DivBackward0>)\n",
      "episode:  1777  loss:  tensor([0.0006], grad_fn=<DivBackward0>)\n",
      "episode:  1778  loss:  tensor([0.0021], grad_fn=<DivBackward0>)\n",
      "episode:  1779  loss:  tensor([-0.0016], grad_fn=<DivBackward0>)\n",
      "episode:  1780  loss:  tensor([0.0435], grad_fn=<DivBackward0>)\n",
      "episode:  1781  loss:  tensor([2.3842e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1782  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1783  loss:  tensor([0.0097], grad_fn=<DivBackward0>)\n",
      "episode:  1784  loss:  tensor([-0.0052], grad_fn=<DivBackward0>)\n",
      "episode:  1785  loss:  tensor([0.0255], grad_fn=<DivBackward0>)\n",
      "episode:  1786  loss:  tensor([0.0222], grad_fn=<DivBackward0>)\n",
      "episode:  1787  loss:  tensor([0.0284], grad_fn=<DivBackward0>)\n",
      "episode:  1788  loss:  tensor([0.0396], grad_fn=<DivBackward0>)\n",
      "episode:  1789  loss:  tensor([0.0261], grad_fn=<DivBackward0>)\n",
      "episode:  1790  loss:  tensor([0.0236], grad_fn=<DivBackward0>)\n",
      "episode:  1791  loss:  tensor([-5.4638e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1792  loss:  tensor([0.0153], grad_fn=<DivBackward0>)\n",
      "episode:  1793  loss:  tensor([-0.0010], grad_fn=<DivBackward0>)\n",
      "episode:  1794  loss:  tensor([0.0328], grad_fn=<DivBackward0>)\n",
      "episode:  1795  loss:  tensor([0.0345], grad_fn=<DivBackward0>)\n",
      "episode:  1796  loss:  tensor([0.0027], grad_fn=<DivBackward0>)\n",
      "episode:  1797  loss:  tensor([0.0099], grad_fn=<DivBackward0>)\n",
      "episode:  1798  loss:  tensor([0.0219], grad_fn=<DivBackward0>)\n",
      "episode:  1799  loss:  tensor([0.0134], grad_fn=<DivBackward0>)\n",
      "episode:  1800  loss:  tensor([0.0181], grad_fn=<DivBackward0>)\n",
      "episode:  1801  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1802  loss:  tensor([0.0051], grad_fn=<DivBackward0>)\n",
      "episode:  1803  loss:  tensor([0.0013], grad_fn=<DivBackward0>)\n",
      "episode:  1804  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  1805  loss:  tensor([-0.0001], grad_fn=<DivBackward0>)\n",
      "episode:  1806  loss:  tensor([-0.0023], grad_fn=<DivBackward0>)\n",
      "episode:  1807  loss:  tensor([0.0018], grad_fn=<DivBackward0>)\n",
      "episode:  1808  loss:  tensor([0.0046], grad_fn=<DivBackward0>)\n",
      "episode:  1809  loss:  tensor([0.0033], grad_fn=<DivBackward0>)\n",
      "episode:  1810  loss:  tensor([0.0041], grad_fn=<DivBackward0>)\n",
      "episode:  1811  loss:  tensor([3.5321e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1812  loss:  tensor([-0.0060], grad_fn=<DivBackward0>)\n",
      "episode:  1813  loss:  tensor([-0.0045], grad_fn=<DivBackward0>)\n",
      "episode:  1814  loss:  tensor([-0.0116], grad_fn=<DivBackward0>)\n",
      "episode:  1815  loss:  tensor([-0.0080], grad_fn=<DivBackward0>)\n",
      "episode:  1816  loss:  tensor([0.6644], grad_fn=<DivBackward0>)\n",
      "episode:  1817  loss:  tensor([0.0066], grad_fn=<DivBackward0>)\n",
      "episode:  1818  loss:  tensor([0.0323], grad_fn=<DivBackward0>)\n",
      "episode:  1819  loss:  tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "episode:  1820  loss:  tensor([0.0822], grad_fn=<DivBackward0>)\n",
      "episode:  1821  loss:  tensor([7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1822  loss:  tensor([0.0289], grad_fn=<DivBackward0>)\n",
      "episode:  1823  loss:  tensor([0.0591], grad_fn=<DivBackward0>)\n",
      "episode:  1824  loss:  tensor([0.0624], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beca\\AppData\\Local\\Temp\\ipykernel_4872\\718079957.py:56: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = valueLoss(reward_preds,discontinued_rewards)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1825  loss:  tensor([0.0750], grad_fn=<DivBackward0>)\n",
      "episode:  1826  loss:  tensor([0.0677], grad_fn=<DivBackward0>)\n",
      "episode:  1827  loss:  tensor([0.0604], grad_fn=<DivBackward0>)\n",
      "episode:  1828  loss:  tensor([0.0388], grad_fn=<DivBackward0>)\n",
      "episode:  1829  loss:  tensor([0.0135], grad_fn=<DivBackward0>)\n",
      "episode:  1830  loss:  tensor([0.0017], grad_fn=<DivBackward0>)\n",
      "episode:  1831  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1832  loss:  tensor([-0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1833  loss:  tensor([0.3133], grad_fn=<DivBackward0>)\n",
      "episode:  1834  loss:  tensor([0.0012], grad_fn=<DivBackward0>)\n",
      "episode:  1835  loss:  tensor([0.0471], grad_fn=<DivBackward0>)\n",
      "episode:  1836  loss:  tensor([0.0867], grad_fn=<DivBackward0>)\n",
      "episode:  1837  loss:  tensor([0.1300], grad_fn=<DivBackward0>)\n",
      "episode:  1838  loss:  tensor([0.1603], grad_fn=<DivBackward0>)\n",
      "episode:  1839  loss:  tensor([0.2141], grad_fn=<DivBackward0>)\n",
      "episode:  1840  loss:  tensor([0.2589], grad_fn=<DivBackward0>)\n",
      "episode:  1841  loss:  tensor([-8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1842  loss:  tensor([0.0220], grad_fn=<DivBackward0>)\n",
      "episode:  1843  loss:  tensor([0.0043], grad_fn=<DivBackward0>)\n",
      "episode:  1844  loss:  tensor([0.0062], grad_fn=<DivBackward0>)\n",
      "episode:  1845  loss:  tensor([-0.0003], grad_fn=<DivBackward0>)\n",
      "episode:  1846  loss:  tensor([-0.0108], grad_fn=<DivBackward0>)\n",
      "episode:  1847  loss:  tensor([-0.0230], grad_fn=<DivBackward0>)\n",
      "episode:  1848  loss:  tensor([-0.0267], grad_fn=<DivBackward0>)\n",
      "episode:  1849  loss:  tensor([1.3182], grad_fn=<DivBackward0>)\n",
      "episode:  1850  loss:  tensor([0.0183], grad_fn=<DivBackward0>)\n",
      "episode:  1851  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1852  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1853  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1854  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1855  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1856  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1857  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1858  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1859  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1860  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1861  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1862  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1863  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1864  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  1865  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1866  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1867  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1868  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1869  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1870  loss:  tensor([1.0366e-07], grad_fn=<DivBackward0>)\n",
      "episode:  1871  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1872  loss:  tensor([-1.1108e-07], grad_fn=<DivBackward0>)\n",
      "episode:  1873  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1874  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1875  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1876  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1877  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  1878  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1879  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1880  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1881  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1882  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1883  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1884  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1885  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1886  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1887  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1888  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1889  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1890  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1891  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1892  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1893  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1894  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1895  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1896  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1897  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1898  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1899  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1900  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1901  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1902  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1903  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1904  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1905  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1906  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1907  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1908  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1909  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1910  loss:  tensor([-2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1911  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1912  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1913  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1914  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1915  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1916  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1917  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1918  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1919  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1920  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1921  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1922  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1923  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1924  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1925  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1926  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1927  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1928  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1929  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1930  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1931  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1932  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1933  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1934  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1935  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1936  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1937  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1938  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1939  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1940  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1941  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1942  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1943  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1944  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1945  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1946  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1947  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1948  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1949  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1950  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1951  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1952  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1953  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1954  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1955  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1956  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1957  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1958  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1959  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1960  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1961  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1962  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1963  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1964  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1965  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1966  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1967  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1968  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1969  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1970  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1971  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1972  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1973  loss:  tensor([-9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1974  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  1975  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1976  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1977  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1978  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1979  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1980  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1981  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1982  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  1983  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1984  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1985  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1986  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1987  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1988  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1989  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1990  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1991  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1992  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1993  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1994  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1995  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1996  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1997  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  1998  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  1999  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2000  loss:  tensor([-2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2001  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2002  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2003  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2004  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2005  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2006  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2007  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2008  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2009  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2010  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2011  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2012  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2013  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2014  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2015  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2016  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2017  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2018  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2019  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2020  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2021  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2022  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2023  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2024  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2025  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2026  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2027  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2028  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2029  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2030  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2031  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2032  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2033  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2034  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2035  loss:  tensor([3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2036  loss:  tensor([-5.2154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2037  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2038  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2039  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2040  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2041  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2042  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2043  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2044  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2045  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2046  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2047  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2048  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2049  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2050  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2051  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2052  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2053  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2054  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2055  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2056  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2057  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2058  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2059  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2060  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2061  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2062  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2063  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2064  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2065  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2066  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2067  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2068  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2069  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2070  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2071  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2072  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2073  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2074  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2075  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2076  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2077  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2078  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2079  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2080  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2081  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2082  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2083  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2084  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2085  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2086  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2087  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2088  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2089  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2090  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2091  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2092  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2093  loss:  tensor([-3.7253e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2094  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2095  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2096  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2097  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2098  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2099  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2100  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2101  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2102  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2103  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2104  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2105  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2106  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2107  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2108  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2109  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2110  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2111  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2112  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2113  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2114  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2115  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2116  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2117  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2118  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2119  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2120  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2121  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2122  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2123  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2124  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2125  loss:  tensor([-9.6858e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2126  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2127  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2128  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2129  loss:  tensor([-9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2130  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2131  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2132  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2133  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2134  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2135  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2136  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2137  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2138  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2139  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2140  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2141  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2142  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2143  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2144  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2145  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2146  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2147  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2148  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2149  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2150  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2151  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2152  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2153  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2154  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2155  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2156  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2157  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2158  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2159  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2160  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2161  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2162  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2163  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2164  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2165  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2166  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2167  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2168  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2169  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2170  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2171  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2172  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2173  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2174  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2175  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2176  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2177  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2178  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2179  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2180  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2181  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2182  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2183  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2184  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2185  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2186  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2187  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2188  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2189  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2190  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2191  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2192  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2193  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2194  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2195  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2196  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2197  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2198  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2199  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2200  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2201  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2202  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2203  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2204  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2205  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2206  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2207  loss:  tensor([1.1650e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2208  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2209  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2210  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2211  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2212  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2213  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2214  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2215  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2216  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2217  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2218  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2219  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2220  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2221  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2222  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2223  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2224  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2225  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2226  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2227  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2228  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2229  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2230  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2231  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2232  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2233  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2234  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2235  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2236  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2237  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2238  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2239  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2240  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2241  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2242  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2243  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2244  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2245  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2246  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2247  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2248  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2249  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2250  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2251  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2252  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2253  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2254  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2255  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2256  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2257  loss:  tensor([7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2258  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2259  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2260  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2261  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2262  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2263  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2264  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2265  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2266  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2267  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2268  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2269  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2270  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2271  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2272  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2273  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2274  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2275  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2276  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2277  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2278  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2279  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2280  loss:  tensor([8.6698e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2281  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2282  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2283  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2284  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2285  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2286  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2287  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2288  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2289  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2290  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2291  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2292  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2293  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2294  loss:  tensor([1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2295  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2296  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2297  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2298  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2299  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2300  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2301  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2302  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2303  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2304  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2305  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2306  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2307  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2308  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2309  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2310  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2311  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2312  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2313  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2314  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2315  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2316  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2317  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2318  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2319  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2320  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2321  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2322  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2323  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2324  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2325  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2326  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2327  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2328  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2329  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2330  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2331  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2332  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2333  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2334  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2335  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2336  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2337  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2338  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2339  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2340  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2341  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2342  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2343  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2344  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2345  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2346  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2347  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2348  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2349  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2350  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2351  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2352  loss:  tensor([-1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2353  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2354  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2355  loss:  tensor([9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2356  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2357  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2358  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2359  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2360  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2361  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2362  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2363  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2364  loss:  tensor([-6.2088e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2365  loss:  tensor([8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2366  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2367  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2368  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2369  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2370  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2371  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2372  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2373  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2374  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2375  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2376  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2377  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2378  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2379  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2380  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2381  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2382  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2383  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2384  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2385  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2386  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2387  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2388  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2389  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2390  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2391  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2392  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2393  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2394  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2395  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2396  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2397  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2398  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2399  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2400  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2401  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2402  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2403  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2404  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2405  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2406  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2407  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2408  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2409  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2410  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2411  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2412  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2413  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2414  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2415  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2416  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2417  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2418  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2419  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2420  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2421  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2422  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2423  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2424  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2425  loss:  tensor([1.0295e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2426  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2427  loss:  tensor([-1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2428  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2429  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2430  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2431  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2432  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2433  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2434  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2435  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2436  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2437  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2438  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2439  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2440  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2441  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2442  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2443  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2444  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2445  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2446  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2447  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2448  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2449  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2450  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2451  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2452  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2453  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2454  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2455  loss:  tensor([8.4440e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2456  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2457  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2458  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2459  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2460  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2461  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2462  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2463  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2464  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2465  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2466  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2467  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2468  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2469  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2470  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2471  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2472  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2473  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2474  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2475  loss:  tensor([-1.0295e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2476  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2477  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2478  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2479  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2480  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2481  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2482  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2483  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2484  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2485  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2486  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2487  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2488  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2489  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2490  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2491  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2492  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2493  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2494  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2495  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2496  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2497  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2498  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2499  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2500  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2501  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2502  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2503  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2504  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2505  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2506  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2507  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2508  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2509  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2510  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2511  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2512  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2513  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2514  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2515  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2516  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2517  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2518  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2519  loss:  tensor([-1.1921e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2520  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2521  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2522  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2523  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2524  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2525  loss:  tensor([1.1143e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2526  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2527  loss:  tensor([-7.9473e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2528  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2529  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2530  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2531  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2532  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2533  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2534  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2535  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2536  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2537  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2538  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2539  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2540  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2541  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2542  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2543  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2544  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2545  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2546  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2547  loss:  tensor([-1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2548  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2549  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2550  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2551  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2552  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2553  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2554  loss:  tensor([7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2555  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2556  loss:  tensor([-4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2557  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2558  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2559  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2560  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2561  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2562  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2563  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2564  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2565  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2566  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2567  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2568  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2569  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2570  loss:  tensor([8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2571  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2572  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2573  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2574  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2575  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2576  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2577  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2578  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2579  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2580  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2581  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2582  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2583  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2584  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2585  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2586  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2587  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2588  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2589  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2590  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2591  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2592  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2593  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2594  loss:  tensor([-9.4374e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2595  loss:  tensor([3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2596  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2597  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2598  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2599  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2600  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2601  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2602  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2603  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2604  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2605  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2606  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2607  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2608  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2609  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2610  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2611  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2612  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2613  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2614  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2615  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2616  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2617  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2618  loss:  tensor([7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2619  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2620  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2621  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2622  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2623  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2624  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2625  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2626  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2627  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2628  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2629  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2630  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2631  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2632  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2633  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2634  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2635  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2636  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2637  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2638  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2639  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2640  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2641  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2642  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2643  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2644  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2645  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2646  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2647  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2648  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2649  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2650  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2651  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2652  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2653  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2654  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2655  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2656  loss:  tensor([1.3005e-07], grad_fn=<DivBackward0>)\n",
      "episode:  2657  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2658  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2659  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2660  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2661  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2662  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2663  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2664  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2665  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2666  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2667  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2668  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2669  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2670  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2671  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2672  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2673  loss:  tensor([8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2674  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2675  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2676  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2677  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2678  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2679  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2680  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2681  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2682  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2683  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2684  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2685  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2686  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2687  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2688  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2689  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2690  loss:  tensor([-8.4440e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2691  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2692  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2693  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2694  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2695  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2696  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2697  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2698  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2699  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2700  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2701  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2702  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2703  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2704  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2705  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2706  loss:  tensor([-2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2707  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2708  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2709  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2710  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2711  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2712  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2713  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2714  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2715  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2716  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2717  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2718  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2719  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2720  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2721  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2722  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2723  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2724  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2725  loss:  tensor([-1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2726  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2727  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2728  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2729  loss:  tensor([8.6698e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2730  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2731  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2732  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2733  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2734  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2735  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2736  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2737  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2738  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2739  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2740  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2741  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2742  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2743  loss:  tensor([-8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2744  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2745  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2746  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2747  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2748  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2749  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2750  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2751  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2752  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2753  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2754  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2755  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2756  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2757  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2758  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2759  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2760  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2761  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2762  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2763  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2764  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2765  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2766  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2767  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2768  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2769  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2770  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2771  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2772  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2773  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2774  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2775  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2776  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2777  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2778  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2779  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2780  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2781  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2782  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2783  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2784  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2785  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2786  loss:  tensor([4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2787  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2788  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2789  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2790  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2791  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2792  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2793  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2794  loss:  tensor([7.2022e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2795  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2796  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2797  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2798  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2799  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2800  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2801  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2802  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2803  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2804  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2805  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2806  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2807  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2808  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2809  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2810  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2811  loss:  tensor([7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2812  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2813  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2814  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2815  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2816  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2817  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2818  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2819  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2820  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2821  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2822  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2823  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2824  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2825  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2826  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2827  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2828  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2829  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2830  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2831  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2832  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2833  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2834  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2835  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2836  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2837  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2838  loss:  tensor([7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2839  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2840  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2841  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2842  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2843  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2844  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2845  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2846  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2847  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2848  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2849  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2850  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2851  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2852  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2853  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2854  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2855  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2856  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2857  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2858  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2859  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2860  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2861  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2862  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2863  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2864  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2865  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2866  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2867  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2868  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2869  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2870  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2871  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2872  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2873  loss:  tensor([1.4901e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2874  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2875  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2876  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2877  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2878  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2879  loss:  tensor([-2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2880  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2881  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2882  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2883  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2884  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2885  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2886  loss:  tensor([7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2887  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2888  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2889  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2890  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2891  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2892  loss:  tensor([6.2088e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2893  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2894  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2895  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2896  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2897  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2898  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2899  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2900  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2901  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2902  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2903  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2904  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2905  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2906  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2907  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2908  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2909  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2910  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2911  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2912  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2913  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2914  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2915  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2916  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2917  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2918  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2919  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2920  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2921  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2922  loss:  tensor([-7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2923  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2924  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2925  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2926  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2927  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2928  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2929  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2930  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2931  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2932  loss:  tensor([-1.4901e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2933  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2934  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2935  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2936  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2937  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2938  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2939  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2940  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2941  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2942  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2943  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2944  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2945  loss:  tensor([-7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2946  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2947  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2948  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2949  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2950  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2951  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2952  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2953  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2954  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2955  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2956  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2957  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2958  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2959  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2960  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2961  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2962  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2963  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2964  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2965  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2966  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2967  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2968  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2969  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2970  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2971  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2972  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2973  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2974  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2975  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2976  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2977  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2978  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2979  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2980  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2981  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2982  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2983  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2984  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2985  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2986  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2987  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2988  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2989  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2990  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2991  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2992  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  2993  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2994  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  2995  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2996  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2997  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2998  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  2999  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3000  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3001  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3002  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3003  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3004  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3005  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3006  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3007  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3008  loss:  tensor([-1.4901e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3009  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3010  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3011  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3012  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3013  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3014  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3015  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3016  loss:  tensor([2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3017  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3018  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3019  loss:  tensor([-1.4253e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3020  loss:  tensor([-7.6989e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3021  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3022  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3023  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3024  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3025  loss:  tensor([4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3026  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3027  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3028  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3029  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3030  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3031  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3032  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3033  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3034  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3035  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3036  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3037  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3038  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3039  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3040  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3041  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3042  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3043  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3044  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3045  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3046  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3047  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3048  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3049  loss:  tensor([-7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3050  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3051  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3052  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3053  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3054  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3055  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3056  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3057  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3058  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3059  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3060  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3061  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3062  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3063  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3064  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3065  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3066  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3067  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3068  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3069  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3070  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3071  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3072  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3073  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3074  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3075  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3076  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3077  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3078  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3079  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3080  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3081  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3082  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3083  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3084  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3085  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3086  loss:  tensor([7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3087  loss:  tensor([3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3088  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3089  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3090  loss:  tensor([-1.0295e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3091  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3092  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3093  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3094  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3095  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3096  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3097  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3098  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3099  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3100  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3101  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3102  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3103  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3104  loss:  tensor([-7.4506e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3105  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3106  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3107  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3108  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3109  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3110  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3111  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3112  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3113  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3114  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3115  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3116  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3117  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3118  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3119  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3120  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3121  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3122  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3123  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3124  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3125  loss:  tensor([8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3126  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3127  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3128  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3129  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3130  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3131  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3132  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3133  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3134  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3135  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3136  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3137  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3138  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3139  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3140  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3141  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3142  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3143  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3144  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3145  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3146  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3147  loss:  tensor([7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3148  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3149  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3150  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3151  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3152  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3153  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3154  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3155  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3156  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3157  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3158  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3159  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3160  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3161  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3162  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3163  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3164  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3165  loss:  tensor([-3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3166  loss:  tensor([-8.4440e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3167  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3168  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3169  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3170  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3171  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3172  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3173  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3174  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3175  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3176  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3177  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3178  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3179  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3180  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3181  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3182  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3183  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3184  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3185  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3186  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3187  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3188  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3189  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3190  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3191  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3192  loss:  tensor([-1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3193  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3194  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3195  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3196  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3197  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3198  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3199  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3200  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3201  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3202  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3203  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3204  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3205  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3206  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3207  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3208  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3209  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3210  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3211  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3212  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3213  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3214  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3215  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3216  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3217  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3218  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3219  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3220  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3221  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3222  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3223  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3224  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3225  loss:  tensor([5.4638e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3226  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3227  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3228  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3229  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3230  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3231  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3232  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3233  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3234  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3235  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3236  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3237  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3238  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3239  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3240  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3241  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3242  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3243  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3244  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3245  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3246  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3247  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3248  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3249  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3250  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3251  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3252  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3253  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3254  loss:  tensor([-1.2698e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3255  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3256  loss:  tensor([7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3257  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3258  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3259  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3260  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3261  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3262  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3263  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3264  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3265  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3266  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3267  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3268  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3269  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3270  loss:  tensor([7.2022e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3271  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3272  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3273  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3274  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3275  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3276  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3277  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3278  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3279  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3280  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3281  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3282  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3283  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3284  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3285  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3286  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3287  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3288  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3289  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3290  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3291  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3292  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3293  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3294  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3295  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3296  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3297  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3298  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3299  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3300  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3301  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3302  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3303  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3304  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3305  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3306  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3307  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3308  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3309  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3310  loss:  tensor([1.2180e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3311  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3312  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3313  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3314  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3315  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3316  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3317  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3318  loss:  tensor([4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3319  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3320  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3321  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3322  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3323  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3324  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3325  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3326  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3327  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3328  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3329  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3330  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3331  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3332  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3333  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3334  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3335  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3336  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3337  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3338  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3339  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3340  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3341  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3342  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3343  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3344  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3345  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3346  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3347  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3348  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3349  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3350  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3351  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3352  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3353  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3354  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3355  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3356  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3357  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3358  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3359  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3360  loss:  tensor([1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3361  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3362  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3363  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3364  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3365  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3366  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3367  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3368  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3369  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3370  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3371  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3372  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3373  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3374  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3375  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3376  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3377  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3378  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3379  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3380  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3381  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3382  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3383  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3384  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3385  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3386  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3387  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3388  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3389  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3390  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3391  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3392  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3393  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3394  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3395  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3396  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3397  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3398  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3399  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3400  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3401  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3402  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3403  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3404  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3405  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3406  loss:  tensor([-1.1379e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3407  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3408  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3409  loss:  tensor([9.8477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3410  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3411  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3412  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3413  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3414  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3415  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3416  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3417  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3418  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3419  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3420  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3421  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3422  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3423  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3424  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3425  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3426  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3427  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3428  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3429  loss:  tensor([-7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3430  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3431  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3432  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3433  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3434  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3435  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3436  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3437  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3438  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3439  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3440  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3441  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3442  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3443  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3444  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3445  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3446  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3447  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3448  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3449  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3450  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3451  loss:  tensor([1.1379e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3452  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3453  loss:  tensor([-8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3454  loss:  tensor([1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3455  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3456  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3457  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3458  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3459  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3460  loss:  tensor([-8.6698e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3461  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3462  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3463  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3464  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3465  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3466  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3467  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3468  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3469  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3470  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3471  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3472  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3473  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3474  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3475  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3476  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3477  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3478  loss:  tensor([-6.4572e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3479  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3480  loss:  tensor([-8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3481  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3482  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3483  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3484  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3485  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3486  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3487  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3488  loss:  tensor([-8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3489  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3490  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3491  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3492  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3493  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3494  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3495  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3496  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3497  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3498  loss:  tensor([-9.8477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3499  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3500  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3501  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3502  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3503  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3504  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3505  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3506  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3507  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3508  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3509  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3510  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3511  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3512  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3513  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3514  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3515  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3516  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3517  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3518  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3519  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3520  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3521  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3522  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3523  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3524  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3525  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3526  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3527  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3528  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3529  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3530  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3531  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3532  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3533  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3534  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3535  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3536  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3537  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3538  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3539  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3540  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3541  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3542  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3543  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3544  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3545  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3546  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3547  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3548  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3549  loss:  tensor([-1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3550  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3551  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3552  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3553  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3554  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3555  loss:  tensor([-1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3556  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3557  loss:  tensor([-4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3558  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3559  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3560  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3561  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3562  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3563  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3564  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3565  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3566  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3567  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3568  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3569  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3570  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3571  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3572  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3573  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3574  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3575  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3576  loss:  tensor([-9.4374e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3577  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3578  loss:  tensor([6.7055e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3579  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3580  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3581  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3582  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3583  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3584  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3585  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3586  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3587  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3588  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3589  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3590  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3591  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3592  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3593  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3594  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3595  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3596  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3597  loss:  tensor([1.6326e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3598  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3599  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3600  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3601  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3602  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3603  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3604  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3605  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3606  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3607  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3608  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3609  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3610  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3611  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3612  loss:  tensor([-1.4901e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3613  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3614  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3615  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3616  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3617  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3618  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3619  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3620  loss:  tensor([-1.2914e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3621  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3622  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3623  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3624  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3625  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3626  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3627  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3628  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3629  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3630  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3631  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3632  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3633  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3634  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3635  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3636  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3637  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3638  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3639  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3640  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3641  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3642  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3643  loss:  tensor([5.4638e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3644  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3645  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3646  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3647  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3648  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3649  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3650  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3651  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3652  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3653  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3654  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3655  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3656  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3657  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3658  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3659  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3660  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3661  loss:  tensor([4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3662  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3663  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3664  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3665  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3666  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3667  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3668  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3669  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3670  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3671  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3672  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3673  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3674  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3675  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3676  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3677  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3678  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3679  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3680  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3681  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3682  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3683  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3684  loss:  tensor([-8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3685  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3686  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3687  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3688  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3689  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3690  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3691  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3692  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3693  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3694  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3695  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3696  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3697  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3698  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3699  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3700  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3701  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3702  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3703  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3704  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3705  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3706  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3707  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3708  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3709  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3710  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3711  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3712  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3713  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3714  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3715  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3716  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3717  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3718  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3719  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3720  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3721  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3722  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3723  loss:  tensor([8.6698e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3724  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3725  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3726  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3727  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3728  loss:  tensor([-6.9539e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3729  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3730  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3731  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3732  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3733  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3734  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3735  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3736  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3737  loss:  tensor([9.4374e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3738  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3739  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3740  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3741  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3742  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3743  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3744  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3745  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3746  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3747  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3748  loss:  tensor([1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3749  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3750  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3751  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3752  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3753  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3754  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3755  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3756  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3757  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3758  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3759  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3760  loss:  tensor([-4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3761  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3762  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3763  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3764  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3765  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3766  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3767  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3768  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3769  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3770  loss:  tensor([1.1921e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3771  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3772  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3773  loss:  tensor([-8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3774  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3775  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3776  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3777  loss:  tensor([8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3778  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3779  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3780  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3781  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3782  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3783  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3784  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3785  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3786  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3787  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3788  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3789  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3790  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3791  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3792  loss:  tensor([7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3793  loss:  tensor([1.2439e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3794  loss:  tensor([-6.4572e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3795  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3796  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3797  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3798  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3799  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3800  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3801  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3802  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3803  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3804  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3805  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3806  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3807  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3808  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3809  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3810  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3811  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3812  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3813  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3814  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3815  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3816  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3817  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3818  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3819  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3820  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3821  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3822  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3823  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3824  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3825  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3826  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3827  loss:  tensor([1.4901e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3828  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3829  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3830  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3831  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3832  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3833  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3834  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3835  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3836  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3837  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3838  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3839  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3840  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3841  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3842  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3843  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3844  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3845  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3846  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3847  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3848  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3849  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3850  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3851  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3852  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3853  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3854  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3855  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3856  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3857  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3858  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3859  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3860  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3861  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3862  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3863  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3864  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3865  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3866  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3867  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3868  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3869  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3870  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3871  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3872  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3873  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3874  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3875  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3876  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3877  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3878  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3879  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3880  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3881  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3882  loss:  tensor([2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3883  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3884  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3885  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3886  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3887  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3888  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3889  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3890  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3891  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3892  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3893  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3894  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3895  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3896  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3897  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3898  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3899  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3900  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3901  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3902  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3903  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3904  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3905  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3906  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3907  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3908  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3909  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3910  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3911  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3912  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3913  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3914  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3915  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3916  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3917  loss:  tensor([1.0366e-07], grad_fn=<DivBackward0>)\n",
      "episode:  3918  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3919  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3920  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3921  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3922  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3923  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3924  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3925  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3926  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3927  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3928  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3929  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3930  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3931  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3932  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3933  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3934  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3935  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3936  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3937  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3938  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3939  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3940  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3941  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3942  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3943  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3944  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3945  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3946  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3947  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3948  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3949  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3950  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3951  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3952  loss:  tensor([9.9341e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3953  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3954  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3955  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3956  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3957  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3958  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3959  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3960  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3961  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3962  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3963  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3964  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3965  loss:  tensor([-1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3966  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3967  loss:  tensor([-2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3968  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3969  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3970  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3971  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3972  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3973  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3974  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3975  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3976  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3977  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3978  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3979  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3980  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  3981  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3982  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3983  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3984  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3985  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3986  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3987  loss:  tensor([7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3988  loss:  tensor([9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3989  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3990  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3991  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3992  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3993  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3994  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3995  loss:  tensor([3.7253e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3996  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3997  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  3998  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  3999  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4000  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4001  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4002  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4003  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4004  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4005  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4006  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4007  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4008  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4009  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4010  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4011  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4012  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4013  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4014  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4015  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4016  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4017  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4018  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4019  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4020  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4021  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4022  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4023  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4024  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4025  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4026  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4027  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4028  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4029  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4030  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4031  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4032  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4033  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4034  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4035  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4036  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4037  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4038  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4039  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4040  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4041  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4042  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4043  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4044  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4045  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4046  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4047  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4048  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4049  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4050  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4051  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4052  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4053  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4054  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4055  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4056  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4057  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4058  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4059  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4060  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4061  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4062  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4063  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4064  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4065  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4066  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4067  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4068  loss:  tensor([4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4069  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4070  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4071  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4072  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4073  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4074  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4075  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4076  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4077  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4078  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4079  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4080  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4081  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4082  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4083  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4084  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4085  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4086  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4087  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4088  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4089  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4090  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4091  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4092  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4093  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4094  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4095  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4096  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4097  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4098  loss:  tensor([2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4099  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4100  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4101  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4102  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4103  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4104  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4105  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4106  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4107  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4108  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4109  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4110  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4111  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4112  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4113  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4114  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4115  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4116  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4117  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4118  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4119  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4120  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4121  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4122  loss:  tensor([-4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4123  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4124  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4125  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4126  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4127  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4128  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4129  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4130  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4131  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4132  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4133  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4134  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4135  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4136  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4137  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4138  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4139  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4140  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4141  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4142  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4143  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4144  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4145  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4146  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4147  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4148  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4149  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4150  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4151  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4152  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4153  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4154  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4155  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4156  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4157  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4158  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4159  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4160  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4161  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4162  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4163  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4164  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4165  loss:  tensor([9.8477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4166  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4167  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4168  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4169  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4170  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4171  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4172  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4173  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4174  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4175  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4176  loss:  tensor([-4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4177  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4178  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4179  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4180  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4181  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4182  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4183  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4184  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4185  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4186  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4187  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4188  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4189  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4190  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4191  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4192  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4193  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4194  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4195  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4196  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4197  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4198  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4199  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4200  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4201  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4202  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4203  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4204  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4205  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4206  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4207  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4208  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4209  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4210  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4211  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4212  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4213  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4214  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4215  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4216  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4217  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4218  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4219  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4220  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4221  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4222  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4223  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4224  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4225  loss:  tensor([-8.9407e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4226  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4227  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4228  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4229  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4230  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4231  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4232  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4233  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4234  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4235  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4236  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4237  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4238  loss:  tensor([-1.0366e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4239  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4240  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4241  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4242  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4243  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4244  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4245  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4246  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4247  loss:  tensor([3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4248  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4249  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4250  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4251  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4252  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4253  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4254  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4255  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4256  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4257  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4258  loss:  tensor([1.0107e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4259  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4260  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4261  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4262  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4263  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4264  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4265  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4266  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4267  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4268  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4269  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4270  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4271  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4272  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4273  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4274  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4275  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4276  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4277  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4278  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4279  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4280  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4281  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4282  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4283  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4284  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4285  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4286  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4287  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4288  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4289  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4290  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4291  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4292  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4293  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4294  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4295  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4296  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4297  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4298  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4299  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4300  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4301  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4302  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4303  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4304  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4305  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4306  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4307  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4308  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4309  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4310  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4311  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4312  loss:  tensor([-1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4313  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4314  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4315  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4316  loss:  tensor([4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4317  loss:  tensor([-1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4318  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4319  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4320  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4321  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4322  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4323  loss:  tensor([-4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4324  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4325  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4326  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4327  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4328  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4329  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4330  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4331  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4332  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4333  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4334  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4335  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4336  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4337  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4338  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4339  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4340  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4341  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4342  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4343  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4344  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4345  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4346  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4347  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4348  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4349  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4350  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4351  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4352  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4353  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4354  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4355  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4356  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4357  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4358  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4359  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4360  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4361  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4362  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4363  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4364  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4365  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4366  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4367  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4368  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4369  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4370  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4371  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4372  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4373  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4374  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4375  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4376  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4377  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4378  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4379  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4380  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4381  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4382  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4383  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4384  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4385  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4386  loss:  tensor([-6.7055e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4387  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4388  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4389  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4390  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4391  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4392  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4393  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4394  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4395  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4396  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4397  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4398  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4399  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4400  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4401  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4402  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4403  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4404  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4405  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4406  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4407  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4408  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4409  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4410  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4411  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4412  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4413  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4414  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4415  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4416  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4417  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4418  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4419  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4420  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4421  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4422  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4423  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4424  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4425  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4426  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4427  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4428  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4429  loss:  tensor([-3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4430  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4431  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4432  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4433  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4434  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4435  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4436  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4437  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4438  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4439  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4440  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4441  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4442  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4443  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4444  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4445  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4446  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4447  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4448  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4449  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4450  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4451  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4452  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4453  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4454  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4455  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4456  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4457  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4458  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4459  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4460  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4461  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4462  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4463  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4464  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4465  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4466  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4467  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4468  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4469  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4470  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4471  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4472  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4473  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4474  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4475  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4476  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4477  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4478  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4479  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4480  loss:  tensor([7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4481  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4482  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4483  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4484  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4485  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4486  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4487  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4488  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4489  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4490  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4491  loss:  tensor([-7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4492  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4493  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4494  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4495  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4496  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4497  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4498  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4499  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4500  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4501  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4502  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4503  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4504  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4505  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4506  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4507  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4508  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4509  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4510  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4511  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4512  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4513  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4514  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4515  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4516  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4517  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4518  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4519  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4520  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4521  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4522  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4523  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4524  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4525  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4526  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4527  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4528  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4529  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4530  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4531  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4532  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4533  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4534  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4535  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4536  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4537  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4538  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4539  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4540  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4541  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4542  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4543  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4544  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4545  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4546  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4547  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4548  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4549  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4550  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4551  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4552  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4553  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4554  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4555  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4556  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4557  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4558  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4559  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4560  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4561  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4562  loss:  tensor([7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4563  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4564  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4565  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4566  loss:  tensor([5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4567  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4568  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4569  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4570  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4571  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4572  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4573  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4574  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4575  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4576  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4577  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4578  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4579  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4580  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4581  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4582  loss:  tensor([1.0679e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4583  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4584  loss:  tensor([-3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4585  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4586  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4587  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4588  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4589  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4590  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4591  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4592  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4593  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4594  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4595  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4596  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4597  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4598  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4599  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4600  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4601  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4602  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4603  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4604  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4605  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4606  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4607  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4608  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4609  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4610  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4611  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4612  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4613  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4614  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4615  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4616  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4617  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4618  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4619  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4620  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4621  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4622  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4623  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4624  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4625  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4626  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4627  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4628  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4629  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4630  loss:  tensor([-1.0366e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4631  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4632  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4633  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4634  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4635  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4636  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4637  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4638  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4639  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4640  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4641  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4642  loss:  tensor([-1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4643  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4644  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4645  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4646  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4647  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4648  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4649  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4650  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4651  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4652  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4653  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4654  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4655  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4656  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4657  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4658  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4659  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4660  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4661  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4662  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4663  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4664  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4665  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4666  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4667  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4668  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4669  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4670  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4671  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4672  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4673  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4674  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4675  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4676  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4677  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4678  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4679  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4680  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4681  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4682  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4683  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4684  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4685  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4686  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4687  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4688  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4689  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4690  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4691  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4692  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4693  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4694  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4695  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4696  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4697  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4698  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4699  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4700  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4701  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4702  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4703  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4704  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4705  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4706  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4707  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4708  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4709  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4710  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4711  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4712  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4713  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4714  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4715  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4716  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4717  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4718  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4719  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4720  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4721  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4722  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4723  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4724  loss:  tensor([2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4725  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4726  loss:  tensor([-1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4727  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4728  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4729  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4730  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4731  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4732  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4733  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4734  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4735  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4736  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4737  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4738  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4739  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4740  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4741  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4742  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4743  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4744  loss:  tensor([1.1921e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4745  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4746  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4747  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4748  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4749  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4750  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4751  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4752  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4753  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4754  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4755  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4756  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4757  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4758  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4759  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4760  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4761  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4762  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4763  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4764  loss:  tensor([7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4765  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4766  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4767  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4768  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4769  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4770  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4771  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4772  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4773  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4774  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4775  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4776  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4777  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4778  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4779  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4780  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4781  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4782  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4783  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4784  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4785  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4786  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4787  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4788  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4789  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4790  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4791  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4792  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4793  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4794  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4795  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4796  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4797  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4798  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4799  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4800  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4801  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4802  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4803  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4804  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4805  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4806  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4807  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4808  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4809  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4810  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4811  loss:  tensor([4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4812  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4813  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4814  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4815  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4816  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4817  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4818  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4819  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4820  loss:  tensor([-7.2022e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4821  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4822  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4823  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4824  loss:  tensor([3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4825  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4826  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4827  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4828  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4829  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4830  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4831  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4832  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4833  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4834  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4835  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4836  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4837  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4838  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4839  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4840  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4841  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4842  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4843  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4844  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4845  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4846  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4847  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4848  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4849  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4850  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4851  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4852  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4853  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4854  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4855  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4856  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4857  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4858  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4859  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4860  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4861  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4862  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4863  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4864  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4865  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4866  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4867  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4868  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4869  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4870  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4871  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4872  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4873  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4874  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4875  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4876  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4877  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4878  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4879  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4880  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4881  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4882  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4883  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4884  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4885  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4886  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4887  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4888  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4889  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4890  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4891  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4892  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4893  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4894  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4895  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4896  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4897  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4898  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4899  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4900  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4901  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4902  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4903  loss:  tensor([8.1956e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4904  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4905  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4906  loss:  tensor([1.3735e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4907  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4908  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4909  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4910  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4911  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4912  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4913  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4914  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4915  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4916  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4917  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4918  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4919  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4920  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4921  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4922  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4923  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4924  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4925  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4926  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4927  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4928  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4929  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4930  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4931  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4932  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4933  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4934  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4935  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4936  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4937  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4938  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4939  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4940  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4941  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4942  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4943  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4944  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4945  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4946  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4947  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4948  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4949  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4950  loss:  tensor([-1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4951  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4952  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4953  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4954  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4955  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4956  loss:  tensor([1.2463e-07], grad_fn=<DivBackward0>)\n",
      "episode:  4957  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4958  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4959  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4960  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4961  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4962  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4963  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4964  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4965  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4966  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4967  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4968  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4969  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4970  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4971  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4972  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4973  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4974  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4975  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4976  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4977  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4978  loss:  tensor([-8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4979  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4980  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4981  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4982  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4983  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4984  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4985  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4986  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4987  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4988  loss:  tensor([-4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4989  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4990  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4991  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4992  loss:  tensor([-5.7121e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4993  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4994  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4995  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4996  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  4997  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  4998  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  4999  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5000  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5001  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5002  loss:  tensor([6.4572e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5003  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5004  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5005  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5006  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5007  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5008  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5009  loss:  tensor([-1.1921e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5010  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5011  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5012  loss:  tensor([-8.6698e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5013  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5014  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5015  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5016  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5017  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5018  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5019  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5020  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5021  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5022  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5023  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5024  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5025  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5026  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5027  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5028  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5029  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5030  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5031  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5032  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5033  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5034  loss:  tensor([7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5035  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5036  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5037  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5038  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5039  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5040  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5041  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5042  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5043  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5044  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5045  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5046  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5047  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5048  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5049  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5050  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5051  loss:  tensor([-1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5052  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5053  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5054  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5055  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5056  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5057  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5058  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5059  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5060  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5061  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5062  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5063  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5064  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5065  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5066  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5067  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5068  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5069  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5070  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5071  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5072  loss:  tensor([-1.0837e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5073  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5074  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5075  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5076  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5077  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5078  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5079  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5080  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5081  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5082  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5083  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5084  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5085  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5086  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5087  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5088  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5089  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5090  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5091  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5092  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5093  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5094  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5095  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5096  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5097  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5098  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5099  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5100  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5101  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5102  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5103  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5104  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5105  loss:  tensor([-7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5106  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5107  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5108  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5109  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5110  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5111  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5112  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5113  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5114  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5115  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5116  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5117  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5118  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5119  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5120  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5121  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5122  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5123  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5124  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5125  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5126  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5127  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5128  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5129  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5130  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5131  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5132  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5133  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5134  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5135  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5136  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5137  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5138  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5139  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5140  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5141  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5142  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5143  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5144  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5145  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5146  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5147  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5148  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5149  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5150  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5151  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5152  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5153  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5154  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5155  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5156  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5157  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5158  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5159  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5160  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5161  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5162  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5163  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5164  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5165  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5166  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5167  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5168  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5169  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5170  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5171  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5172  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5173  loss:  tensor([-1.0024e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5174  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5175  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5176  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5177  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5178  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5179  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5180  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5181  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5182  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5183  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5184  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5185  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5186  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5187  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5188  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5189  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5190  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5191  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5192  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5193  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5194  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5195  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5196  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5197  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5198  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5199  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5200  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5201  loss:  tensor([1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5202  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5203  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5204  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5205  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5206  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5207  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5208  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5209  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5210  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5211  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5212  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5213  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5214  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5215  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5216  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5217  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5218  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5219  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5220  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5221  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5222  loss:  tensor([-8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5223  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5224  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5225  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5226  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5227  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5228  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5229  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5230  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5231  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5232  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5233  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5234  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5235  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5236  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5237  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5238  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5239  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5240  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5241  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5242  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5243  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5244  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5245  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5246  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5247  loss:  tensor([-2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5248  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5249  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5250  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5251  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5252  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5253  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5254  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5255  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5256  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5257  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5258  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5259  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5260  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5261  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5262  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5263  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5264  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5265  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5266  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5267  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5268  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5269  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5270  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5271  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5272  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5273  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5274  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5275  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5276  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5277  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5278  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5279  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5280  loss:  tensor([-7.4506e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5281  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5282  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5283  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5284  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5285  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5286  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5287  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5288  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5289  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5290  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5291  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5292  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5293  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5294  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5295  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5296  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5297  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5298  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5299  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5300  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5301  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5302  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5303  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5304  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5305  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5306  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5307  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5308  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5309  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5310  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5311  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5312  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5313  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5314  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5315  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5316  loss:  tensor([-4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5317  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5318  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5319  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5320  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5321  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5322  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5323  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5324  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5325  loss:  tensor([-8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5326  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5327  loss:  tensor([9.7535e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5328  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5329  loss:  tensor([9.1890e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5330  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5331  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5332  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5333  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5334  loss:  tensor([2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5335  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5336  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5337  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5338  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5339  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5340  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5341  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5342  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5343  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5344  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5345  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5346  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5347  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5348  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5349  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5350  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5351  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5352  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5353  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5354  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5355  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5356  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5357  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5358  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5359  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5360  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5361  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5362  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5363  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5364  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5365  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5366  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5367  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5368  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5369  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5370  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5371  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5372  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5373  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5374  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5375  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5376  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5377  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5378  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5379  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5380  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5381  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5382  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5383  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5384  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5385  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5386  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5387  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5388  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5389  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5390  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5391  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5392  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5393  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5394  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5395  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5396  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5397  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5398  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5399  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5400  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5401  loss:  tensor([7.2022e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5402  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5403  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5404  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5405  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5406  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5407  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5408  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5409  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5410  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5411  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5412  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5413  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5414  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5415  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5416  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5417  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5418  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5419  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5420  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5421  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5422  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5423  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5424  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5425  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5426  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5427  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5428  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5429  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5430  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5431  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5432  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5433  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5434  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5435  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5436  loss:  tensor([1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5437  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5438  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5439  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5440  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5441  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5442  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5443  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5444  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5445  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5446  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5447  loss:  tensor([8.4440e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5448  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5449  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5450  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5451  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5452  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5453  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5454  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5455  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5456  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5457  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5458  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5459  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5460  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5461  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5462  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5463  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5464  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5465  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5466  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5467  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5468  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5469  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5470  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5471  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5472  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5473  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5474  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5475  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5476  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5477  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5478  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5479  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5480  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5481  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5482  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5483  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5484  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5485  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5486  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5487  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5488  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5489  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5490  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5491  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5492  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5493  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5494  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5495  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5496  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5497  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5498  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5499  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5500  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5501  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5502  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5503  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5504  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5505  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5506  loss:  tensor([1.7385e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5507  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5508  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5509  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5510  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5511  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5512  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5513  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5514  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5515  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5516  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5517  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5518  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5519  loss:  tensor([-1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5520  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5521  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5522  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5523  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5524  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5525  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5526  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5527  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5528  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5529  loss:  tensor([1.2439e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5530  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5531  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5532  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5533  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5534  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5535  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5536  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5537  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5538  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5539  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5540  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5541  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5542  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5543  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5544  loss:  tensor([3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5545  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5546  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5547  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5548  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5549  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5550  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5551  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5552  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5553  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5554  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5555  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5556  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5557  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5558  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5559  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5560  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5561  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5562  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5563  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5564  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5565  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5566  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5567  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5568  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5569  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5570  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5571  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5572  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5573  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5574  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5575  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5576  loss:  tensor([-9.5886e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5577  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5578  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5579  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5580  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5581  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5582  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5583  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5584  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5585  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5586  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5587  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5588  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5589  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5590  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5591  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5592  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5593  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5594  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5595  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5596  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5597  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5598  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5599  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5600  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5601  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5602  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5603  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5604  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5605  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5606  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5607  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5608  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5609  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5610  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5611  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5612  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5613  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5614  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5615  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5616  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5617  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5618  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5619  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5620  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5621  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5622  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5623  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5624  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5625  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5626  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5627  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5628  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5629  loss:  tensor([-2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5630  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5631  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5632  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5633  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5634  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5635  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5636  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5637  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5638  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5639  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5640  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5641  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5642  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5643  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5644  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5645  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5646  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5647  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5648  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5649  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5650  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5651  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5652  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5653  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5654  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5655  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5656  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5657  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5658  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5659  loss:  tensor([-6.4572e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5660  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5661  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5662  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5663  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5664  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5665  loss:  tensor([1.0366e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5666  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5667  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5668  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5669  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5670  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5671  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5672  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5673  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5674  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5675  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5676  loss:  tensor([8.1279e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5677  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5678  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5679  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5680  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5681  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5682  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5683  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5684  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5685  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5686  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5687  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5688  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5689  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5690  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5691  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5692  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5693  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5694  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5695  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5696  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5697  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5698  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5699  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5700  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5701  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5702  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5703  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5704  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5705  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5706  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5707  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5708  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5709  loss:  tensor([1.2192e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5710  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5711  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5712  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5713  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5714  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5715  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5716  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5717  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5718  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5719  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5720  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5721  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5722  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5723  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5724  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5725  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5726  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5727  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5728  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5729  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5730  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5731  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5732  loss:  tensor([3.2286e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5733  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5734  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5735  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5736  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5737  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5738  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5739  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5740  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5741  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5742  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5743  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5744  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5745  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5746  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5747  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5748  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5749  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5750  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5751  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5752  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5753  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5754  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5755  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5756  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5757  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5758  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5759  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5760  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5761  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5762  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5763  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5764  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5765  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5766  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5767  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5768  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5769  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5770  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5771  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5772  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5773  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5774  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5775  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5776  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5777  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5778  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5779  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5780  loss:  tensor([-7.5860e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5781  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5782  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5783  loss:  tensor([1.0107e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5784  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5785  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5786  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5787  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5788  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5789  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5790  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5791  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5792  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5793  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5794  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5795  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5796  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5797  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5798  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5799  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5800  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5801  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5802  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5803  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5804  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5805  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5806  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5807  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5808  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5809  loss:  tensor([6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5810  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5811  loss:  tensor([2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5812  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5813  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5814  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5815  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5816  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5817  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5818  loss:  tensor([-1.0295e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5819  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5820  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5821  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5822  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5823  loss:  tensor([-1.0107e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5824  loss:  tensor([-1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5825  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5826  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5827  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5828  loss:  tensor([-8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5829  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5830  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5831  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5832  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5833  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5834  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5835  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5836  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5837  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5838  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5839  loss:  tensor([1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5840  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5841  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5842  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5843  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5844  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5845  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5846  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5847  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5848  loss:  tensor([2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5849  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5850  loss:  tensor([5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5851  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5852  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5853  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5854  loss:  tensor([-5.4638e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5855  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5856  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5857  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5858  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5859  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5860  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5861  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5862  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5863  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5864  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5865  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5866  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5867  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5868  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5869  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5870  loss:  tensor([1.0024e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5871  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5872  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5873  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5874  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5875  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5876  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5877  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5878  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5879  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5880  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5881  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5882  loss:  tensor([-1.0295e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5883  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5884  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5885  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5886  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5887  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5888  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5889  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5890  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5891  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5892  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5893  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5894  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5895  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5896  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5897  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5898  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5899  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5900  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5901  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5902  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5903  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5904  loss:  tensor([-2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5905  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5906  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5907  loss:  tensor([-1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5908  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5909  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5910  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5911  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5912  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5913  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5914  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5915  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5916  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5917  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5918  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5919  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5920  loss:  tensor([4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5921  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5922  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5923  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  5924  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5925  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5926  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5927  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5928  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5929  loss:  tensor([4.2220e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5930  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5931  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5932  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5933  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5934  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5935  loss:  tensor([-1.5808e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5936  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5937  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5938  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5939  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5940  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5941  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5942  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5943  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5944  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5945  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5946  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5947  loss:  tensor([6.7055e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5948  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5949  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5950  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5951  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5952  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5953  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5954  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5955  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5956  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5957  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5958  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5959  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5960  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5961  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5962  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5963  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5964  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5965  loss:  tensor([-4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5966  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5967  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5968  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5969  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5970  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5971  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5972  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5973  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5974  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5975  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5976  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5977  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5978  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5979  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5980  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5981  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5982  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5983  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5984  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5985  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5986  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5987  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5988  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5989  loss:  tensor([-1.0107e-07], grad_fn=<DivBackward0>)\n",
      "episode:  5990  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5991  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5992  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5993  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5994  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5995  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5996  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  5997  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5998  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  5999  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6000  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6001  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6002  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6003  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6004  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6005  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6006  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6007  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6008  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6009  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6010  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6011  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6012  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6013  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6014  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6015  loss:  tensor([-1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6016  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6017  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6018  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6019  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6020  loss:  tensor([4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6021  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6022  loss:  tensor([-4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6023  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6024  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6025  loss:  tensor([1.1403e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6026  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6027  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6028  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6029  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6030  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6031  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6032  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6033  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6034  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6035  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6036  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6037  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6038  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6039  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6040  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6041  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6042  loss:  tensor([2.2352e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6043  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6044  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6045  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6046  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6047  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6048  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6049  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6050  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6051  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6052  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6053  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6054  loss:  tensor([9.4374e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6055  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6056  loss:  tensor([-9.7535e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6057  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6058  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6059  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6060  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6061  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6062  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6063  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6064  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6065  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6066  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6067  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6068  loss:  tensor([8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6069  loss:  tensor([-4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6070  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6071  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6072  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6073  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6074  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6075  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6076  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6077  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6078  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6079  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6080  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6081  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6082  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6083  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6084  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6085  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6086  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6087  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6088  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6089  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6090  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6091  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6092  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6093  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6094  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6095  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6096  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6097  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6098  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6099  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6100  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6101  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6102  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6103  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6104  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6105  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6106  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6107  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6108  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6109  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6110  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6111  loss:  tensor([-7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6112  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6113  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6114  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6115  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6116  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6117  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6118  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6119  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6120  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6121  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6122  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6123  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6124  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6125  loss:  tensor([7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6126  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6127  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6128  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6129  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6130  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6131  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6132  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6133  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6134  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6135  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6136  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6137  loss:  tensor([-3.7253e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6138  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6139  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6140  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6141  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6142  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6143  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6144  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6145  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6146  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6147  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6148  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6149  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6150  loss:  tensor([8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6151  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6152  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6153  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6154  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6155  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6156  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6157  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6158  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6159  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6160  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6161  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6162  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6163  loss:  tensor([-9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6164  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6165  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6166  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6167  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6168  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6169  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6170  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6171  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6172  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6173  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6174  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6175  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6176  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6177  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6178  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6179  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6180  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6181  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6182  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6183  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6184  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6185  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6186  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6187  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6188  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6189  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6190  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6191  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6192  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6193  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6194  loss:  tensor([-1.0884e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6195  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6196  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6197  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6198  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6199  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6200  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6201  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6202  loss:  tensor([-1.0431e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6203  loss:  tensor([-2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6204  loss:  tensor([-3.4769e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6205  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6206  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6207  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6208  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6209  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6210  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6211  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6212  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6213  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6214  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6215  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6216  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6217  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6218  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6219  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6220  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6221  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6222  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6223  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6224  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6225  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6226  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6227  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6228  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6229  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6230  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6231  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6232  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6233  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6234  loss:  tensor([9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6235  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6236  loss:  tensor([9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6237  loss:  tensor([8.3988e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6238  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6239  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6240  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6241  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6242  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6243  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6244  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6245  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6246  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6247  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6248  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6249  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6250  loss:  tensor([6.7055e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6251  loss:  tensor([-2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6252  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6253  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6254  loss:  tensor([-4.7187e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6255  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6256  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6257  loss:  tensor([4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6258  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6259  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6260  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6261  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6262  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6263  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6264  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6265  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6266  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6267  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6268  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6269  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6270  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6271  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6272  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6273  loss:  tensor([-1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6274  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6275  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6276  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6277  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6278  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6279  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6280  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6281  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6282  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6283  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6284  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6285  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6286  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6287  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6288  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6289  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6290  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6291  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6292  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6293  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6294  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6295  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6296  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6297  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6298  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6299  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6300  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6301  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6302  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6303  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6304  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6305  loss:  tensor([1.1403e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6306  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6307  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6308  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6309  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6310  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6311  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6312  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6313  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6314  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6315  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6316  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6317  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6318  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6319  loss:  tensor([-6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6320  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6321  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6322  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6323  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6324  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6325  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6326  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6327  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6328  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6329  loss:  tensor([-7.3151e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6330  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6331  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6332  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6333  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6334  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6335  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6336  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6337  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6338  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6339  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6340  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6341  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6342  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6343  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6344  loss:  tensor([-4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6345  loss:  tensor([1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6346  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6347  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6348  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6349  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6350  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6351  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6352  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6353  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6354  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6355  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6356  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6357  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6358  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6359  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6360  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6361  loss:  tensor([-7.0442e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6362  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6363  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6364  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6365  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6366  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6367  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6368  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6369  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6370  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6371  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6372  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6373  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6374  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6375  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6376  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6377  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6378  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6379  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6380  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6381  loss:  tensor([4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6382  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6383  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6384  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6385  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6386  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6387  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6388  loss:  tensor([-5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6389  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6390  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6391  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6392  loss:  tensor([4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6393  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6394  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6395  loss:  tensor([-3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6396  loss:  tensor([3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6397  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6398  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6399  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6400  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6401  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6402  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6403  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6404  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6405  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6406  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6407  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6408  loss:  tensor([1.1379e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6409  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6410  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6411  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6412  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6413  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6414  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6415  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6416  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6417  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6418  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6419  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6420  loss:  tensor([1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6421  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6422  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6423  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6424  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6425  loss:  tensor([1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6426  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6427  loss:  tensor([-4.9671e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6428  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6429  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6430  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6431  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6432  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6433  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6434  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6435  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6436  loss:  tensor([1.0107e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6437  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6438  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6439  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6440  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6441  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6442  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6443  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6444  loss:  tensor([5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6445  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6446  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6447  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6448  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6449  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6450  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6451  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6452  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6453  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6454  loss:  tensor([-5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6455  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6456  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6457  loss:  tensor([-3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6458  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6459  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6460  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6461  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6462  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6463  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6464  loss:  tensor([6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6465  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6466  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6467  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6468  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6469  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6470  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6471  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6472  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6473  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6474  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6475  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6476  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6477  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6478  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6479  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6480  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6481  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6482  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6483  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6484  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6485  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6486  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6487  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6488  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6489  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6490  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6491  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6492  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6493  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6494  loss:  tensor([9.2116e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6495  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6496  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6497  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6498  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6499  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6500  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6501  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6502  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6503  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6504  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6505  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6506  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6507  loss:  tensor([-1.2180e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6508  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6509  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6510  loss:  tensor([-1.1662e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6511  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6512  loss:  tensor([4.0640e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6513  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6514  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6515  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6516  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6517  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6518  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6519  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6520  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6521  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6522  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6523  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6524  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6525  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6526  loss:  tensor([-2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6527  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6528  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6529  loss:  tensor([-8.5520e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6530  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6531  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6532  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6533  loss:  tensor([-8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6534  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6535  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6536  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6537  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6538  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6539  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6540  loss:  tensor([7.2022e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6541  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6542  loss:  tensor([1.9868e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6543  loss:  tensor([-2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6544  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6545  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6546  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6547  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6548  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6549  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6550  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6551  loss:  tensor([6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6552  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6553  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6554  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6555  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6556  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6557  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6558  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6559  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6560  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6561  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6562  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6563  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6564  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6565  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6566  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6567  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6568  loss:  tensor([-5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6569  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6570  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6571  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6572  loss:  tensor([-1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6573  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6574  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6575  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6576  loss:  tensor([-3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6577  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6578  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6579  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6580  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6581  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6582  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6583  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6584  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6585  loss:  tensor([9.4826e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6586  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6587  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6588  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6589  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6590  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6591  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6592  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6593  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6594  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6595  loss:  tensor([2.7319e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6596  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6597  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6598  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6599  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6600  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6601  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6602  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6603  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6604  loss:  tensor([2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6605  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6606  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6607  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6608  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6609  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6610  loss:  tensor([7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6611  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6612  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6613  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6614  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6615  loss:  tensor([3.5221e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6616  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6617  loss:  tensor([7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6618  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6619  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6620  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6621  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6622  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6623  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6624  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6625  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6626  loss:  tensor([-6.5023e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6627  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6628  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6629  loss:  tensor([9.3294e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6630  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6631  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6632  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6633  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6634  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6635  loss:  tensor([9.8477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6636  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6637  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6638  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6639  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6640  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6641  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6642  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6643  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6644  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6645  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6646  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6647  loss:  tensor([8.8111e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6648  loss:  tensor([-2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6649  loss:  tensor([5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6650  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6651  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6652  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6653  loss:  tensor([-7.5154e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6654  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6655  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6656  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6657  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6658  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6659  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6660  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6661  loss:  tensor([-4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6662  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6663  loss:  tensor([-3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6664  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6665  loss:  tensor([-4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6666  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6667  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6668  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6669  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6670  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6671  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6672  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6673  loss:  tensor([-4.4703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6674  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6675  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6676  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6677  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6678  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6679  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6680  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6681  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6682  loss:  tensor([-7.2562e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6683  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6684  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6685  loss:  tensor([8.2928e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6686  loss:  tensor([-2.4835e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6687  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6688  loss:  tensor([-9.9341e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6689  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6690  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6691  loss:  tensor([-7.7745e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6692  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6693  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6694  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6695  loss:  tensor([-6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6696  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6697  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6698  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6699  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6700  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6701  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6702  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6703  loss:  tensor([6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6704  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6705  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6706  loss:  tensor([-4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6707  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6708  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6709  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6710  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6711  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6712  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6713  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6714  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6715  loss:  tensor([2.7093e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6716  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6717  loss:  tensor([-1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6718  loss:  tensor([1.2958e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6719  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6720  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6721  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6722  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6723  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6724  loss:  tensor([-1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6725  loss:  tensor([5.4186e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6726  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6727  loss:  tensor([5.1830e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6728  loss:  tensor([5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6729  loss:  tensor([-3.9736e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6730  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6731  loss:  tensor([4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6732  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6733  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6734  loss:  tensor([5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6735  loss:  tensor([3.7253e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6736  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6737  loss:  tensor([7.8570e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6738  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6739  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6740  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6741  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6742  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6743  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6744  loss:  tensor([9.2116e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6745  loss:  tensor([7.6989e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6746  loss:  tensor([6.7733e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6747  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6748  loss:  tensor([-5.4186e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6749  loss:  tensor([1.0625e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6750  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6751  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6752  loss:  tensor([2.1674e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6753  loss:  tensor([3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6754  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6755  loss:  tensor([-1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6756  loss:  tensor([-5.9605e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6757  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6758  loss:  tensor([-4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6759  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6760  loss:  tensor([2.0732e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6761  loss:  tensor([4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6762  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6763  loss:  tensor([-9.0703e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6764  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6765  loss:  tensor([1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6766  loss:  tensor([-4.3349e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6767  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6768  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6769  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6770  loss:  tensor([2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6771  loss:  tensor([-3.8873e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6772  loss:  tensor([-2.4835e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6773  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6774  loss:  tensor([-2.5915e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6775  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6776  loss:  tensor([5.6895e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6777  loss:  tensor([1.2418e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6778  loss:  tensor([9.2116e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6779  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6780  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6781  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6782  loss:  tensor([3.2512e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6783  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6784  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6785  loss:  tensor([-2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6786  loss:  tensor([-1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6787  loss:  tensor([-6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6788  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6789  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6790  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6791  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6792  loss:  tensor([6.4788e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6793  loss:  tensor([-1.3217e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6794  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6795  loss:  tensor([4.6647e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6796  loss:  tensor([-1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6797  loss:  tensor([-5.4422e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6798  loss:  tensor([4.1464e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6799  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6800  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6801  loss:  tensor([-7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6802  loss:  tensor([1.0837e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6803  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6804  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6805  loss:  tensor([-8.0337e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6806  loss:  tensor([-4.4056e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6807  loss:  tensor([4.6058e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6808  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6809  loss:  tensor([3.7930e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6810  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6811  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6812  loss:  tensor([3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6813  loss:  tensor([-6.9971e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6814  loss:  tensor([1.3547e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6815  loss:  tensor([-3.1098e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6816  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6817  loss:  tensor([-1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6818  loss:  tensor([-2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6819  loss:  tensor([3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6820  loss:  tensor([1.0366e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6821  loss:  tensor([-2.8507e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6822  loss:  tensor([5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6823  loss:  tensor([-2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6824  loss:  tensor([-3.3690e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6825  loss:  tensor([-6.7379e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6826  loss:  tensor([4.9671e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6827  loss:  tensor([2.5915e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6828  loss:  tensor([1.8965e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6829  loss:  tensor([4.8767e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6830  loss:  tensor([-5.7013e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6831  loss:  tensor([8.1279e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6832  loss:  tensor([2.3324e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6833  loss:  tensor([-5.1477e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6834  loss:  tensor([3.6281e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6835  loss:  tensor([4.9239e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6836  loss:  tensor([-1.5549e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6837  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6838  loss:  tensor([0.], grad_fn=<DivBackward0>)\n",
      "episode:  6839  loss:  tensor([2.9802e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6840  loss:  tensor([-5.1830e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6841  loss:  tensor([2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6842  loss:  tensor([1.3476e-07], grad_fn=<DivBackward0>)\n",
      "episode:  6843  loss:  tensor([-2.7093e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6844  loss:  tensor([2.4384e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6845  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6846  loss:  tensor([7.7745e-09], grad_fn=<DivBackward0>)\n",
      "episode:  6847  loss:  tensor([1.6256e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6848  loss:  tensor([6.2314e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6849  loss:  tensor([1.8141e-08], grad_fn=<DivBackward0>)\n",
      "episode:  6850  loss:  tensor([-6.2196e-08], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m     loss = confidence_ratio* advantage \n\u001b[32m     87\u001b[39m     loss2 =torch.clip( (confidence_ratio) , \u001b[32m1\u001b[39m-epsilon , \u001b[32m1\u001b[39m+epsilon)*advantage \n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     loss = -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     total_loss += loss \n\u001b[32m     91\u001b[39m total_loss /= \u001b[38;5;28mlen\u001b[39m(observations)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def is_terminal(observation):\n",
    "    x,v,theta , omega = observation \n",
    "    x_terminal = x <= -4.8 or x>=4.8\n",
    "    theta_terminal = theta <= -0.209*7.5 or theta >= 0.209*7.5\n",
    "    return x_terminal or theta_terminal\n",
    "\n",
    "\n",
    "episodes = 10000\n",
    "\n",
    "value_optimizer = torch.optim.Adam(valueModel.parameters(), lr=1e-3)\n",
    "policy_optimizer = torch.optim.Adam(policyModel.parameters(),lr=1e-3)\n",
    "valueLoss = torch.nn.functional.mse_loss\n",
    "gamma = 0.99\n",
    "epsilon =0.3\n",
    "update_old_step = 10\n",
    "update_old_policy()\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    obs = torch.tensor(obs)\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    confidences= []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            \n",
    "            observations.append(obs)\n",
    "            logits = policyModel(obs)\n",
    "            probs = torch.softmax(logits,dim=-1)\n",
    "\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "            confidences.append(probs[action]) \n",
    "            actions.append(action)\n",
    "            obs, reward, terminated, truncated , info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            obs = torch.tensor(obs)\n",
    "            done = is_terminal(obs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        discontinued_rewards = []\n",
    "        reward = 0 \n",
    "        observation_tensor = torch.stack(observations) \n",
    "        for i in range(-1,-len(rewards) -1,-1):\n",
    "            reward = rewards[i] + gamma * reward\n",
    "            discontinued_rewards.append(reward)\n",
    "\n",
    "\n",
    "        discontinued_rewards.reverse()\n",
    "        discontinued_rewards = torch.tensor(discontinued_rewards,dtype=torch.float32) \n",
    "  \n",
    "    \n",
    "    reward_preds = valueModel(observation_tensor)   \n",
    "    loss = valueLoss(reward_preds,discontinued_rewards)\n",
    "    value_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    value_optimizer.step()\n",
    "    reward_preds = reward_preds.detach()\n",
    "\n",
    "    total_loss = 0\n",
    "    advantages = []\n",
    "    for i in range(len(observations)):\n",
    "        predicted_reward = reward_preds[i]\n",
    "        reward = discontinued_rewards[i]\n",
    "        advantage = reward - predicted_reward \n",
    "        advantages.append(advantage)\n",
    "    advantages = torch.stack(advantages)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std(unbiased=False) + 1e-8)\n",
    "    for i in range(len(observations)):\n",
    "        advantage = advantages[i]\n",
    "        current_logits = policyModel(observations[i])\n",
    "        current_log_probs = torch.log_softmax(current_logits,dim=-1)\n",
    "\n",
    "        current_confidence = current_log_probs[actions[i]]\n",
    "\n",
    "        old_logits = oldPolicyModel(observations[i])\n",
    "       \n",
    "       \n",
    "        old_log_probs = torch.log_softmax(old_logits,dim=-1)\n",
    "\n",
    "        old_confidence =old_log_probs[actions[i]]\n",
    "        \n",
    "        confidence_ratio = torch.exp(current_confidence - old_confidence)\n",
    "        loss = confidence_ratio* advantage \n",
    "        loss2 =torch.clip( (confidence_ratio) , 1-epsilon , 1+epsilon)*advantage \n",
    "        loss = -torch.min(loss, loss2)\n",
    "        \n",
    "        total_loss += loss \n",
    "    total_loss /= len(observations)\n",
    "    print('episode: ',episode + 1, ' loss: ',total_loss)\n",
    "    policy_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    if  (episode + 1)  % update_old_step == 0:\n",
    "        update_old_policy()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b4dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\",render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0414ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:214: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1\n",
      "episode:  2\n",
      "episode:  3\n",
      "episode:  4\n",
      "episode:  5\n",
      "episode:  6\n",
      "episode:  7\n",
      "episode:  8\n",
      "episode:  9\n",
      "episode:  10\n",
      "episode:  11\n",
      "episode:  12\n",
      "episode:  13\n",
      "episode:  14\n",
      "episode:  15\n",
      "episode:  16\n",
      "episode:  17\n",
      "episode:  18\n",
      "episode:  19\n",
      "episode:  20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m confidences.append(probs[action])\n\u001b[32m     18\u001b[39m actions.append(action)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m obs, reward, terminated, truncated , info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m rewards.append(reward)\n\u001b[32m     21\u001b[39m obs = torch.tensor(obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    114\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m    116\u001b[39m \n\u001b[32m    117\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:223\u001b[39m, in \u001b[36mCartPoleEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    220\u001b[39m     reward = -\u001b[32m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sutton_barto_reward \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[38;5;28mself\u001b[39m.state, dtype=np.float32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:337\u001b[39m, in \u001b[36mCartPoleEnv.render\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    336\u001b[39m     pygame.event.pump()\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrender_fps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     pygame.display.flip()\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mrgb_array\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    obs, info = env.reset(seed=42)\n",
    "    obs = torch.tensor(obs)\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    confidences= []\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            \n",
    "            observations.append(obs)\n",
    "            logits = policyModel(obs)\n",
    "            probs = torch.softmax(logits,dim=-1)\n",
    "\n",
    "            action = torch.multinomial(probs, num_samples=1).item()\n",
    "            confidences.append(probs[action])\n",
    "            actions.append(action)\n",
    "            obs, reward, terminated, truncated , info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            obs = torch.tensor(obs)\n",
    "            done = is_terminal(obs)\n",
    "        print('episode: ',episode + 1)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2b5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c806f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
