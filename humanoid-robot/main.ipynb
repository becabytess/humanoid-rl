{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym \n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.distributions.normal import Normal \n",
    "from copy import deepcopy\n",
    "\n",
    "env = gym.make(\"Humanoid-v5\",render_mode='human')\n",
    "action_shape = env.action_space.shape[0]\n",
    "observation_shape = env.observation_space.shape[0]\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(observation_shape,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_shape)\n",
    "        )\n",
    "        self.std = nn.Parameter(torch.zeros(action_shape))\n",
    "\n",
    "    def forward(self,X,action=None):\n",
    "        mean = self.mean(X)\n",
    "        std = torch.exp(self.std)\n",
    "        dist = Normal(mean,std)\n",
    "    \n",
    "        if action == None:\n",
    "            action = dist.sample()  \n",
    "        \n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "\n",
    "        return action ,log_prob ,dist.entropy().sum(dim=-1)\n",
    "\n",
    "class Value(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(observation_shape,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        return self.model(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode:  0\n",
      "starting episode:  1\n",
      "starting episode:  2\n",
      "starting episode:  3\n",
      "starting episode:  4\n",
      "starting episode:  5\n",
      "starting episode:  6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (17,)) of distribution Normal(loc: torch.Size([17]), scale: torch.Size([17])) to satisfy the constraint Real(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         action,prob,entropy =  \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# probs = torch.softmax(logits,dim=-1)\u001b[39;00m\n\u001b[32m     35\u001b[39m         \n\u001b[32m     36\u001b[39m         \u001b[38;5;66;03m# action_index = torch.multinomial(probs,num_samples=1)\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;66;03m# action = torch.zeros(logits.shape[-1])\u001b[39;00m\n\u001b[32m     38\u001b[39m         \u001b[38;5;66;03m# action[action_index]=1\u001b[39;00m\n\u001b[32m     39\u001b[39m         observation, reward , terminated , truncated , info  = env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mPolicy.forward\u001b[39m\u001b[34m(self, X, action)\u001b[39m\n\u001b[32m     27\u001b[39m mean = \u001b[38;5;28mself\u001b[39m.mean(X)\n\u001b[32m     28\u001b[39m std = torch.exp(\u001b[38;5;28mself\u001b[39m.std)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m dist = \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     32\u001b[39m     action = dist.sample()  \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\distributions\\normal.py:59\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     58\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\beca\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\distributions\\distribution.py:71\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter loc (Tensor of shape (17,)) of distribution Normal(loc: torch.Size([17]), scale: torch.Size([17])) to satisfy the constraint Real(), but found invalid values:\ntensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "policy = Policy()\n",
    "\n",
    "old_policy = deepcopy(policy)\n",
    "value_model = Value()\n",
    "def update_old_policy():\n",
    "    global old_policy\n",
    "    old_policy = deepcopy(policy)\n",
    "    old_policy.requires_grad_=False\n",
    "\n",
    "\n",
    "\n",
    "value_optimizer = torch.optim.Adam(value_model.parameters(),lr=1e-3,weight_decay=0.001)\n",
    "policy_optimizer = torch.optim.Adam(policy.parameters(),lr=1e-3,weight_decay=0.001)\n",
    "\n",
    "episodes = 1000\n",
    "\n",
    "gamma = 0.001\n",
    "eps = 0.7 \n",
    "lamda = 0.99 \n",
    "for episode in range(episodes):\n",
    "    update_old_policy()\n",
    "    print('starting episode: ',episode)\n",
    "    done = False\n",
    "    observation,info = env.reset()\n",
    "    observation = torch.tensor(observation,dtype=torch.float32)\n",
    "    actions = []\n",
    "    observations = []\n",
    "    confidences = []\n",
    "    rewards = []\n",
    "    entropies = []\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            action,prob,entropy =  policy(observation)\n",
    "            # probs = torch.softmax(logits,dim=-1)\n",
    "            \n",
    "            # action_index = torch.multinomial(probs,num_samples=1)\n",
    "            # action = torch.zeros(logits.shape[-1])\n",
    "            # action[action_index]=1\n",
    "            observation, reward , terminated , truncated , info  = env.step(action)\n",
    "            entropies.append(entropy)\n",
    "            observation = torch.tensor(observation,dtype=torch.float32)\n",
    "            observations.append(observation)\n",
    "            actions.append(action) \n",
    "           \n",
    "            confidences.append(prob)\n",
    "            rewards.append(reward)\n",
    "            done = terminated \n",
    "            env.render()\n",
    "\n",
    "\n",
    "    reward = 0 \n",
    "    discontinued_rewards = []\n",
    "    for i in range(-1,-len(observations)-1,-1):\n",
    "        reward = rewards[i] + lamda*reward \n",
    "        discontinued_rewards.append(reward)\n",
    "    discontinued_rewards.reverse()\n",
    "    discontinued_rewards = torch.tensor(discontinued_rewards,dtype=torch.float32)\n",
    "\n",
    "    observations = torch.stack(observations)\n",
    "\n",
    "    #train the value model \n",
    "    reward_preds = value_model(observations)\n",
    "    loss = torch.nn.functional.mse_loss(reward_preds.squeeze(-1),discontinued_rewards)\n",
    "    value_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    value_optimizer.step()\n",
    "    #advantages \n",
    "    with torch.no_grad():\n",
    "        advs = []\n",
    "        for i  in range(len(observations)):\n",
    "            reward = discontinued_rewards[i]\n",
    "            pred = value_model(observations[i])\n",
    "            adv = reward - pred \n",
    "            advs.append(adv)\n",
    "        advs = torch.tensor(advs)\n",
    "        advs = (advs - advs.mean())/(advs.std() + 1e-8)\n",
    "    \n",
    "    #train the policy model\n",
    "    \n",
    "\n",
    "    actions = torch.stack(actions)\n",
    "    with torch.no_grad():\n",
    "        _,old_conf,_ = old_policy(observations,actions)\n",
    "    _,current_log_probs,entropies = policy(observations,actions)\n",
    "    conf_ratio = torch.exp(current_log_probs - old_conf)\n",
    "    clipped_conf_ratio = torch.clip(conf_ratio, 1-eps,1+eps)\n",
    "    objective = torch.min(clipped_conf_ratio*advs , conf_ratio*advs)\n",
    "    \n",
    "    # entropies = torch.stack(entropies)\n",
    "    loss = (-objective) - gamma*entropies\n",
    "    total_loss = loss.mean()\n",
    "    policy_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    policy_optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
